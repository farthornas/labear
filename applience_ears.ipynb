{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# Read metadata file\ndata_paths = {'appliences':'/kaggle/input/appliences/appliences/','urban':'/kaggle/input/urbansound8k/'}\nmeta1 = data_paths['appliences'] + 'metadata/appliences.csv'\nmeta2 = data_paths['urban'] + 'UrbanSound8K.csv'\ndf_UrbanSound = pd.read_csv(meta2)\ndf_Appliences = pd.read_csv(meta1)\nupsmpl = int(df_UrbanSound.classID.value_counts().max() / df_Appliences.classID.value_counts().min())\ndf_Appliences = df_Appliences.sample(frac=upsmpl, replace=True, random_state=1)\ndf_UrbanSound['relative_path'] = '/fold' + df_UrbanSound['fold'].astype(str) + '/' + df_UrbanSound['slice_file_name'].astype(str)\ndf_Appliences['relative_path'] = '/' + df_Appliences['fold'].astype(str) + '/' + df_Appliences['slice_file_name'].astype(str)\n\ndf = pd.concat([df_UrbanSound,df_Appliences])\ndf = df.reset_index(drop=True)\ndf.head()\n\n#Take relative columns\ndf = df[['relative_path', 'classID']]\ndf.head()\n#print((df.classID==1).sum())\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-06T17:28:18.253199Z","iopub.execute_input":"2023-08-06T17:28:18.254237Z","iopub.status.idle":"2023-08-06T17:28:18.322920Z","shell.execute_reply.started":"2023-08-06T17:28:18.254179Z","shell.execute_reply":"2023-08-06T17:28:18.320746Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"               relative_path  classID\n0    /fold5/100032-3-0-0.wav        3\n1  /fold5/100263-2-0-117.wav        2\n2  /fold5/100263-2-0-121.wav        2\n3  /fold5/100263-2-0-126.wav        2\n4  /fold5/100263-2-0-137.wav        2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>relative_path</th>\n      <th>classID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/fold5/100032-3-0-0.wav</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/fold5/100263-2-0-117.wav</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/fold5/100263-2-0-121.wav</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/fold5/100263-2-0-126.wav</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/fold5/100263-2-0-137.wav</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import math, random\nimport torch\nimport torchaudio\nfrom torchaudio import transforms\nfrom IPython.display import Audio\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nclass AudioUtil():\n    #---------------\n    # Load an audio file. Return the signal as a tensor and the sample rate\n    #---------------\n    @staticmethod\n    def open(audio_file):\n        sig, sr = torchaudio.load(audio_file)\n        return (sig, sr)\n    \n    #---------------\n    # Convert soundfile to desired number of channels\n    #---------------\n    @staticmethod\n    def rechannel(aud, new_channel):\n        \n        sig, sr = aud\n        \n        if sig.shape[0] == new_channel:\n            #Nothing todo\n            return aud\n        \n        if (new_channel == 1):\n            #Convert stereo to mono by selecting only the first channel\n            resig = sig[:1, :]\n        else:\n            #Convert from mono to sterio by duplicating the first channel\n            resig = torch.cat([sig,sig])\n        return ((resig, sr))\n    \n    #---------------\n    #Resample to make sure samplerate is the same for all files - resample applies to one channel at a time\n    #---------------\n    @staticmethod\n    def resample(aud, newsr):\n        \n        sig, sr = aud\n        \n        if (sr == newsr):\n            #do nothing\n            return aud\n        \n        num_channels = sig.shape[0]\n        \n        #resample first channel\n        resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n        \n        if (num_channels > 1):\n            #Resample the second channel and merge both\n            retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n            resig = torch.cat([resig, retwo])\n            \n        return ((resig, newsr))\n    \n    \n    #-----------------\n    #Pad or turncate the signal to be off a standard length in milliseconds\n    #-----------------\n    @staticmethod\n    def pad_trunc(aud, max_ms):\n\n        sig, sr = aud\n        num_rows, sig_len = sig.shape\n        max_len = sr//1000 * max_ms\n        \n        if (sig_len > max_len):\n            #Turncate the signal to the given length\n            sig = sig[:,:max_len]\n        elif (sig_len < max_len):\n            pad_begin_len = random.randint(0, max_len - sig_len)\n            pad_end_len = max_len - sig_len - pad_begin_len\n            \n            #pad with zeroes\n            pad_begin = torch.zeros((num_rows, pad_begin_len))\n            pad_end = torch.zeros((num_rows, pad_end_len))\n            \n            sig = torch.cat((pad_begin, sig, pad_end), 1)\n        \n        return (sig, sr)\n\n    #--------------------\n    #Shift the signal by a random bit, end of signal is wrapped around \n    #to beginning\n    #--------------------\n    @staticmethod\n    def time_shift(aud, shift_limit):\n        sig, sr = aud\n        \n        _, sig_len = sig.shape\n        shift_amt = int(random.random() * shift_limit * sig_len)\n        return (sig.roll(shift_amt), sr)\n    \n    #----------------------------------\n    #Genetate spectrogram\n    #----------------------------------\n    @staticmethod\n    def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n        sig,sr = aud\n        top_db = 80\n        \n        #spec has shape [channel, n_mels, time]\n        spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n        \n        #convert to db\n        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n        \n        return spec\n    \n    \n    #Augment the spectrogram by masking out some sections of it in both the frequency\n    #dimencion (Horizontal) and the time dimension (vertical bars)\n    \n    @staticmethod\n    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n        _, n_mels, n_steps = spec.shape\n        mask_value = spec.mean()\n        aug_spec = spec\n        \n        freq_mask_param = max_mask_pct * n_mels\n        for _ in range(n_freq_masks):\n            aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n        \n        time_mask_param = max_mask_pct * n_steps\n        for _ in range(n_time_masks):\n            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n        \n        return np.absolute(aug_spec) # Jonas added np.absolute\n    \n        ","metadata":{"execution":{"iopub.status.busy":"2023-08-06T16:20:15.520507Z","iopub.execute_input":"2023-08-06T16:20:15.521014Z","iopub.status.idle":"2023-08-06T16:20:19.825519Z","shell.execute_reply.started":"2023-08-06T16:20:15.520975Z","shell.execute_reply":"2023-08-06T16:20:19.824622Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset, random_split\nimport torchaudio\n\n#----------------\n#Sound dataset\n#----------------\n    \nclass SoundDS(Dataset):\n    def __init__(self, df, data_paths):\n        self.df = df\n        self.data_paths = data_paths\n        self.duration = 4000\n        self.sr = 44100\n        self.channel = 2\n        self.shift_pct = 0.4\n    \n    #------------------\n    #Number of items in dataset\n    #------------------\n    def __len__(self):\n        return len(self.df)\n\n    #------------------\n    #Get i'th item in dataset\n    #------------------\n    def __getitem__(self, idx):\n\n        # Get the class ID  \n        class_id = self.df.loc[idx, 'classID']\n        # Absolute file path of the audio file - concatenate the audio direcory\n        # with the relative path\n        if class_id <= df_UrbanSound['classID'].max():  # UrbanSound data set (classIDs 0 - 10)\n            audio_file = self.data_paths['urban'] + self.df.loc[idx, 'relative_path']\n        else: # Appliences (Class IDs => 10)\n            audio_file = self.data_paths['appliences'] + self.df.loc[idx, 'relative_path']\n              \n        aud = AudioUtil.open(audio_file)\n        #Make all the sounds have the same number of channels and same sample rate\n        #Then make all samples the same length\n        reaud = AudioUtil.resample(aud, self.sr)\n        rechan = AudioUtil.rechannel(reaud, self.channel)\n        \n        dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n        shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n        sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n        #aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n        aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n        return aug_sgram, class_id\n","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:39:18.211305Z","iopub.execute_input":"2023-08-06T17:39:18.211786Z","iopub.status.idle":"2023-08-06T17:39:18.224780Z","shell.execute_reply.started":"2023-08-06T17:39:18.211747Z","shell.execute_reply":"2023-08-06T17:39:18.223450Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# Splitt data for training and validation\n\nmyds = SoundDS(df, data_paths) #returns spectrogram and classID\n#Random split between training and validation data\n\nnum_items = len(myds)\nnum_train = round(num_items*0.8)\nnum_val = num_items - num_train\ntrain_ds, val_ds = random_split(myds, [num_train, num_val])\n\n# Create training and validation data loaders\ntrain_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\nval_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:39:21.840031Z","iopub.execute_input":"2023-08-06T17:39:21.840428Z","iopub.status.idle":"2023-08-06T17:39:21.849588Z","shell.execute_reply.started":"2023-08-06T17:39:21.840401Z","shell.execute_reply":"2023-08-06T17:39:21.847821Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\nfrom torch.nn import init\nfrom torch import nn\n\n#---------------------------\n# Audio Classification Model\n#---------------------------\nclass AudioClassifier (nn.Module):\n    #---------------------------\n    #Build the model architecture\n    #---------------------------\n    \n    def __init__(self):\n        super().__init__()\n        conv_layers = []\n        \n        #First convolution block with Relu and Batch Norm. Use Kaiming Initialisation\n        self.conv1 = nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n        self.relu1 = nn.ReLU()\n        self.bn1 = nn.BatchNorm2d(8)\n        init.kaiming_normal_(self.conv1.weight, a=0.1)\n        self.conv1.bias.data.zero_()\n        conv_layers += [self.conv1, self.relu1, self.bn1]\n\n        #Second convolution block\n        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.relu2 = nn.ReLU()\n        self.bn2 = nn.BatchNorm2d(16)\n        init.kaiming_normal_(self.conv2.weight, a=0.1)\n        self.conv2.bias.data.zero_()\n        conv_layers += [self.conv2, self.relu2, self.bn2]\n        \n        #Third convolution block\n        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.relu3 = nn.ReLU()\n        self.bn3 = nn.BatchNorm2d(32)\n        init.kaiming_normal_(self.conv3.weight, a=0.1)\n        self.conv3.bias.data.zero_()\n        conv_layers += [self.conv3, self.relu3, self.bn3]\n\n        #Fourth convolution block\n        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.relu4 = nn.ReLU()\n        self.bn4 = nn.BatchNorm2d(64)\n        init.kaiming_normal_(self.conv4.weight, a=0.1)\n        self.conv4.bias.data.zero_()\n        conv_layers += [self.conv4, self.relu4, self.bn4] \n        \n        # Linear classifier\n        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n        self.lin = nn.Linear(in_features=64, out_features=df.classID.max()+1) #Changed Jonas out_features\n        \n        # Wrap the convolutional blocks\n        self.conv = nn.Sequential(*conv_layers)\n    \n    #-------------------------\n    #Forward pass computations\n    #-------------------------\n    def forward(self, x):\n        # Run convolutional blocks\n        x = self.conv(x)\n        \n        # Adaptive pool and flatten for input to linear layer\n        x = self.ap(x)\n        x = x.view(x.shape[0], -1)\n        \n        # Linear layer\n        x = self.lin(x)\n        \n        # Final output\n        \n        return x\n\n# Create the model and put it on the GPU if available\nmyModel =  AudioClassifier()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmyModel = myModel.to(device)\n#Check that it is on Cuda\nnext(myModel.parameters()).device","metadata":{"execution":{"iopub.status.busy":"2023-08-06T16:35:08.685729Z","iopub.execute_input":"2023-08-06T16:35:08.686138Z","iopub.status.idle":"2023-08-06T16:35:08.764154Z","shell.execute_reply.started":"2023-08-06T16:35:08.686109Z","shell.execute_reply":"2023-08-06T16:35:08.762463Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"device(type='cpu')"},"metadata":{}}]},{"cell_type":"code","source":"#-------------------\n#Training loop\n#-------------------\ndef training(model, train_dl, num_epochs):\n    #Loss function optimiser and Scheduler\n    criterion = nn.CrossEntropyLoss()\n    optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimiser, max_lr=0.001,\n                                                    steps_per_epoch=int(len(train_dl)),\n                                                    epochs=num_epochs,\n                                                    anneal_strategy='linear')\n    \n    #Repeat for each epoch\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        correct_prediction = 0\n        total_prediction = 0\n        \n        #repeat for each batch in the training set\n        for i, data in enumerate(train_dl):\n            # Get the input features and target labels and put them on the GPU\n            inputs, labels = data[0].to(device), data[1].to(device)\n            \n            #Normalise the inputs\n            inputs_m, inputs_s = inputs.mean(), inputs.std()\n            inputs = (inputs - inputs_m) / inputs_s\n            \n            #Zero prameter gradients\n            optimiser.zero_grad()\n            \n            #Forward + backward + optimise\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimiser.step()\n            scheduler.step()\n            \n            #Keep stats for Loss and Accuracy\n            running_loss += loss.item()\n            \n            \n            #Get predicted class with highest score\n            _, prediction = torch.max(outputs, 1)\n            # Count the predictions that matched the target label\n            correct_prediction += (prediction == labels).sum().item()\n            total_prediction += prediction.shape[0]\n            \n            if i % 10 == 0: #\n                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n        \n        #Print stats at the end of each epoch\n        num_batches = len(train_dl)\n        avg_loss = running_loss / num_batches\n        acc = correct_prediction / total_prediction\n        print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n    \n    print('Finished Training')\n    \nnum_epochs = 2\ntraining(myModel, train_dl, num_epochs)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-08-06T16:35:53.586384Z","iopub.execute_input":"2023-08-06T16:35:53.587083Z","iopub.status.idle":"2023-08-06T16:43:51.953024Z","shell.execute_reply.started":"2023-08-06T16:35:53.587043Z","shell.execute_reply":"2023-08-06T16:43:51.951552Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"[1,     1] loss: 0.254\n[1,    11] loss: 2.765\n[1,    21] loss: 5.275\n[1,    31] loss: 7.735\n[1,    41] loss: 10.204\n[1,    51] loss: 12.621\n[1,    61] loss: 15.024\n[1,    71] loss: 17.380\n[1,    81] loss: 19.611\n[1,    91] loss: 21.876\n[1,   101] loss: 24.113\n[1,   111] loss: 26.243\n[1,   121] loss: 28.354\n[1,   131] loss: 30.315\n[1,   141] loss: 32.455\n[1,   151] loss: 34.425\n[1,   161] loss: 36.378\n[1,   171] loss: 38.364\n[1,   181] loss: 40.125\n[1,   191] loss: 42.013\n[1,   201] loss: 43.817\n[1,   211] loss: 45.600\n[1,   221] loss: 47.244\n[1,   231] loss: 48.986\n[1,   241] loss: 50.659\n[1,   251] loss: 52.335\n[1,   261] loss: 54.031\n[1,   271] loss: 55.760\n[1,   281] loss: 57.478\n[1,   291] loss: 59.157\n[1,   301] loss: 60.754\n[1,   311] loss: 62.368\n[1,   321] loss: 63.912\n[1,   331] loss: 65.508\n[1,   341] loss: 67.097\n[1,   351] loss: 68.571\n[1,   361] loss: 70.130\n[1,   371] loss: 71.619\n[1,   381] loss: 73.103\n[1,   391] loss: 74.660\n[1,   401] loss: 76.077\n[1,   411] loss: 77.546\n[1,   421] loss: 78.872\n[1,   431] loss: 80.168\n[1,   441] loss: 81.740\n[1,   451] loss: 83.210\n[1,   461] loss: 84.556\n[1,   471] loss: 85.885\n[1,   481] loss: 87.435\n[1,   491] loss: 88.998\n[1,   501] loss: 90.373\n[1,   511] loss: 91.682\n[1,   521] loss: 93.010\n[1,   531] loss: 94.306\n[1,   541] loss: 95.722\n[1,   551] loss: 97.163\n[1,   561] loss: 98.570\n[1,   571] loss: 100.023\n[1,   581] loss: 101.336\n[1,   591] loss: 102.575\n[1,   601] loss: 103.955\n[1,   611] loss: 105.271\n[1,   621] loss: 106.546\n[1,   631] loss: 107.838\n[1,   641] loss: 109.120\nEpoch: 0, Loss: 1.70, Accuracy: 0.44\n[2,     1] loss: 0.109\n[2,    11] loss: 1.252\n[2,    21] loss: 2.659\n[2,    31] loss: 3.839\n[2,    41] loss: 5.005\n[2,    51] loss: 6.254\n[2,    61] loss: 7.694\n[2,    71] loss: 9.024\n[2,    81] loss: 10.284\n[2,    91] loss: 11.523\n[2,   101] loss: 12.848\n[2,   111] loss: 14.156\n[2,   121] loss: 15.469\n[2,   131] loss: 16.729\n[2,   141] loss: 17.893\n[2,   151] loss: 19.242\n[2,   161] loss: 20.513\n[2,   171] loss: 21.606\n[2,   181] loss: 22.817\n[2,   191] loss: 24.009\n[2,   201] loss: 25.083\n[2,   211] loss: 26.370\n[2,   221] loss: 27.665\n[2,   231] loss: 28.913\n[2,   241] loss: 30.111\n[2,   251] loss: 31.489\n[2,   261] loss: 32.684\n[2,   271] loss: 33.980\n[2,   281] loss: 35.392\n[2,   291] loss: 36.533\n[2,   301] loss: 37.683\n[2,   311] loss: 38.900\n[2,   321] loss: 39.978\n[2,   331] loss: 41.221\n[2,   341] loss: 42.459\n[2,   351] loss: 43.675\n[2,   361] loss: 44.977\n[2,   371] loss: 46.186\n[2,   381] loss: 47.455\n[2,   391] loss: 48.626\n[2,   401] loss: 49.925\n[2,   411] loss: 51.147\n[2,   421] loss: 52.299\n[2,   431] loss: 53.450\n[2,   441] loss: 54.819\n[2,   451] loss: 56.145\n[2,   461] loss: 57.249\n[2,   471] loss: 58.331\n[2,   481] loss: 59.485\n[2,   491] loss: 60.570\n[2,   501] loss: 61.647\n[2,   511] loss: 62.804\n[2,   521] loss: 63.987\n[2,   531] loss: 65.183\n[2,   541] loss: 66.398\n[2,   551] loss: 67.598\n[2,   561] loss: 68.788\n[2,   571] loss: 69.992\n[2,   581] loss: 71.202\n[2,   591] loss: 72.311\n[2,   601] loss: 73.427\n[2,   611] loss: 74.535\n[2,   621] loss: 75.772\n[2,   631] loss: 76.984\n[2,   641] loss: 78.101\nEpoch: 1, Loss: 1.22, Accuracy: 0.58\nFinished Training\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\n\n#-------------------\n#Inferrence\n#-------------------\n\ndef inferrence (model, v_dl):\n    correct_prediction = 0\n    total_prediction = 0\n    \n    #Disable gradient updates\n    with torch.no_grad():\n        for data in v_dl:\n            # Get the input features and target labels and put them on the GPU\n            inputs, labels = data[0].to(device), data[1].to(device)\n            #Normalise the inputs\n            inputs_m, inputs_s = inputs.mean(), inputs.std()\n            inputs = (inputs - inputs_m) / inputs_s\n            \n            #Get predictions\n            outputs = model(inputs)\n            \n            #Get predicted putputs with highest score\n            i, prediction = torch.max(outputs,1)\n            for n in range(len(outputs)):\n                if labels[n].item() == -1:\n                    print(\"Predicted class:{}, actual class:{}\".format(prediction[n].item(), labels[n].item()))\n                else:\n                    if prediction[n].item() == labels[n].item():\n                        df_metr['correct_pred'].loc[prediction[n].item()] += 1\n                    else:\n                        df_metr['false_pred'].loc[prediction[n].item()] += 1\n                #print(\"Predicted class:{}, score:{}, actual class:{}\".format(prediction[n].item(), outputs[n].max().item(), labels[n].item()))\n            #print(outputs[15].max(),prediction[15], labels[15])\n            #Count of predictions that matched target label\n            correct_prediction += (prediction == labels).sum().item()\n            total_prediction += prediction.shape[0]\n            \n    acc = correct_prediction / total_prediction\n    df_metr['accuracy'] = 1 - (df_metr['false_pred'] / df_metr['correct_pred'])\n    print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')\n\n\n\n    \n    \nclassIDs = sorted(list(df.classID.unique()))\ndf_metr = pd.DataFrame({'classIDs': classIDs,\n                        'correct_pred': np.zeros(len(classIDs)),\n                        'false_pred': np.zeros(len(classIDs)),\n                        'accuracy': np.zeros(len(classIDs))})\ndf_metr = df_metr.set_index('classIDs')\n\n#Run inferrence on trained model with validation set\ninferrence(myModel, val_dl)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:39:43.823806Z","iopub.execute_input":"2023-08-06T17:39:43.824327Z","iopub.status.idle":"2023-08-06T17:40:49.665532Z","shell.execute_reply.started":"2023-08-06T17:39:43.824287Z","shell.execute_reply":"2023-08-06T17:40:49.662639Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"          correct_pred  false_pred  accuracy\nclassIDs                                    \n0                  0.0         0.0       0.0\n1                  0.0         0.0       0.0\n2                  0.0         0.0       0.0\n3                  0.0         0.0       0.0\n4                  0.0         0.0       0.0\n5                  0.0         0.0       0.0\n6                  0.0         0.0       0.0\n7                  0.0         0.0       0.0\n8                  0.0         0.0       0.0\n9                  0.0         0.0       0.0\n10                 0.0         0.0       0.0\n11                 0.0         0.0       0.0\n12                 0.0         0.0       0.0\nAccuracy: 0.60, Total items: 2592\n","output_type":"stream"}]},{"cell_type":"code","source":"print(df_metr)","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:40:49.668434Z","iopub.execute_input":"2023-08-06T17:40:49.668859Z","iopub.status.idle":"2023-08-06T17:40:49.681548Z","shell.execute_reply.started":"2023-08-06T17:40:49.668822Z","shell.execute_reply":"2023-08-06T17:40:49.679851Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"          correct_pred  false_pred  accuracy\nclassIDs                                    \n0                 68.0        80.0 -0.176471\n1                 33.0        41.0 -0.242424\n2                 76.0       140.0 -0.842105\n3                 92.0       135.0 -0.467391\n4                 60.0        87.0 -0.450000\n5                111.0       140.0 -0.261261\n6                 19.0        20.0 -0.052632\n7                126.0       136.0 -0.079365\n8                103.0       101.0  0.019417\n9                 65.0       112.0 -0.723077\n10               273.0        11.0  0.959707\n11               348.0        11.0  0.968391\n12               186.0        18.0  0.903226\n","output_type":"stream"}]},{"cell_type":"code","source":"df.loc[5284,:]","metadata":{"execution":{"iopub.status.busy":"2023-08-06T17:05:12.379974Z","iopub.execute_input":"2023-08-06T17:05:12.380468Z","iopub.status.idle":"2023-08-06T17:05:12.401304Z","shell.execute_reply.started":"2023-08-06T17:05:12.380426Z","shell.execute_reply":"2023-08-06T17:05:12.398426Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"relative_path    /fold5/203654-9-0-42.wav\nclassID                                 9\nName: 5284, dtype: object"},"metadata":{}}]}]}