{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff72041a-0c66-4a49-aab3-01551bbf59bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path \n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26b04ac1-594a-40b0-940a-e983b6c2c320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.5 (main, Aug 24 2023, 15:09:45) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n"
     ]
    }
   ],
   "source": [
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b88c0027-8ab1-458b-a405-bf0add29fa37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Kaggle notebook? False\n"
     ]
    }
   ],
   "source": [
    "in_kaggle = 'KAGGLE_USER_SECRETS_TOKEN' in os.environ\n",
    "print(f\"Running in Kaggle notebook? {in_kaggle}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "699a9efa-26a7-47be-b8f2-d1aff351c7dd",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.116828,
     "end_time": "2023-08-09T15:45:29.272233",
     "exception": false,
     "start_time": "2023-08-09T15:45:29.155405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dir_path = Path('')\n",
    "if in_kaggle: dir_path = Path('/kaggle/input/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a418850-c468-402e-9882-6c0b8df1f3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function pandas.io.parsers.readers.read_csv(filepath_or_buffer: 'FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str]', *, sep: 'str | None | lib.NoDefault' = <no_default>, delimiter: 'str | None | lib.NoDefault' = None, header: \"int | Sequence[int] | None | Literal['infer']\" = 'infer', names: 'Sequence[Hashable] | None | lib.NoDefault' = <no_default>, index_col: 'IndexLabel | Literal[False] | None' = None, usecols: 'list[HashableT] | Callable[[Hashable], bool] | None' = None, dtype: 'DtypeArg | None' = None, engine: 'CSVEngine | None' = None, converters: 'Mapping[Hashable, Callable] | None' = None, true_values: 'list | None' = None, false_values: 'list | None' = None, skipinitialspace: 'bool' = False, skiprows: 'list[int] | int | Callable[[Hashable], bool] | None' = None, skipfooter: 'int' = 0, nrows: 'int | None' = None, na_values: 'Sequence[str] | Mapping[str, Sequence[str]] | None' = None, keep_default_na: 'bool' = True, na_filter: 'bool' = True, verbose: 'bool' = False, skip_blank_lines: 'bool' = True, parse_dates: 'bool | Sequence[Hashable] | None' = None, infer_datetime_format: 'bool | lib.NoDefault' = <no_default>, keep_date_col: 'bool' = False, date_parser: 'Callable | lib.NoDefault' = <no_default>, date_format: 'str | None' = None, dayfirst: 'bool' = False, cache_dates: 'bool' = True, iterator: 'bool' = False, chunksize: 'int | None' = None, compression: 'CompressionOptions' = 'infer', thousands: 'str | None' = None, decimal: 'str' = '.', lineterminator: 'str | None' = None, quotechar: 'str' = '\"', quoting: 'int' = 0, doublequote: 'bool' = True, escapechar: 'str | None' = None, comment: 'str | None' = None, encoding: 'str | None' = None, encoding_errors: 'str | None' = 'strict', dialect: 'str | csv.Dialect | None' = None, on_bad_lines: 'str' = 'error', delim_whitespace: 'bool' = False, low_memory: 'bool' = True, memory_map: 'bool' = False, float_precision: \"Literal['high', 'legacy'] | None\" = None, storage_options: 'StorageOptions | None' = None, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>) -> 'DataFrame | TextFileReader'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8625c07-874a-46c7-ab42-4c8e0094edce",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.116828,
     "end_time": "2023-08-09T15:45:29.272233",
     "exception": false,
     "start_time": "2023-08-09T15:45:29.155405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relative_path</th>\n",
       "      <th>classID</th>\n",
       "      <th>synth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>urbansound8k/fold5/100032-3-0-0.wav</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>urbansound8k/fold5/100263-2-0-117.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>urbansound8k/fold5/100263-2-0-121.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>urbansound8k/fold5/100263-2-0-126.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>urbansound8k/fold5/100263-2-0-137.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           relative_path  classID synth\n",
       "0    urbansound8k/fold5/100032-3-0-0.wav        3   NaN\n",
       "1  urbansound8k/fold5/100263-2-0-117.wav        2   NaN\n",
       "2  urbansound8k/fold5/100263-2-0-121.wav        2   NaN\n",
       "3  urbansound8k/fold5/100263-2-0-126.wav        2   NaN\n",
       "4  urbansound8k/fold5/100263-2-0-137.wav        2   NaN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read metadata file\n",
    "\n",
    "\n",
    "\n",
    "data_paths = {\n",
    "    'appliances':dir_path/'appliances',         \n",
    "    'urban':dir_path/'urbansound8k'\n",
    "}\n",
    "\n",
    "meta2 = str(data_paths['urban']) + '/' + 'UrbanSound8K.csv'\n",
    "\n",
    "meta1 = str(data_paths['appliances']) + '/' + 'appliances.csv'\n",
    "df_UrbanSound = pd.read_csv(meta2)\n",
    "df_Appliences = pd.read_csv(meta1)\n",
    "#smpl = df_UrbanSound.classID.value_counts().max() / df_Appliences.classID.value_counts().min()\n",
    "df_App_synth = df_Appliences.sample(frac=30, replace=True, random_state=1)\n",
    "#df_UrbanSound = df_UrbanSound.sample(frac=(1/smpl), replace=True, random_state=1)\n",
    "\n",
    "df_UrbanSound['relative_path'] = str(data_paths[\"urban\"]) + '/fold' + df_UrbanSound['fold'].astype(str) + '/' + df_UrbanSound['slice_file_name'].astype(str)\n",
    "df_Appliences['relative_path'] = str(data_paths[\"appliances\"]) + '/' + df_Appliences['fold'].astype(str) + '/' + df_Appliences['slice_file_name'].astype(str)\n",
    "df_App_synth['relative_path'] = str(data_paths[\"appliances\"]) + '/' + df_Appliences['fold'].astype(str) + '/' + df_Appliences['slice_file_name'].astype(str)\n",
    "df_App_synth[\"synth\"] = True\n",
    "df = pd.concat([df_UrbanSound, df_Appliences, df_App_synth])\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()\n",
    "\n",
    "#Take relative columns\n",
    "df = df[['relative_path', 'classID', \"synth\"]]\n",
    "#df = df[['relative_path', 'classID']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72657450",
   "metadata": {
    "papermill": {
     "duration": 0.014995,
     "end_time": "2023-08-09T15:45:29.292854",
     "exception": false,
     "start_time": "2023-08-09T15:45:29.277859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classID\n",
      "3     1000\n",
      "2     1000\n",
      "0     1000\n",
      "9     1000\n",
      "5     1000\n",
      "7     1000\n",
      "4     1000\n",
      "8      929\n",
      "11     598\n",
      "10     510\n",
      "1      429\n",
      "6      374\n",
      "12     349\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.classID.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75e2f97a-32d7-4cca-98b0-2addf9dcf612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "def rand_effect_gen():\n",
    "    # generate random effect: 0 - echo, 1 - vibrato\n",
    "    effect = random.randint(0,1)\n",
    "    if effect ==0:\n",
    "        delay = random.randint(40,70)\n",
    "        decay = round(random.uniform(0.1,0.5), 2)\n",
    "        effect = \"aecho=in_gain=0.8:out_gain=0.9:delays={}:decays={}\".format(delay, decay)\n",
    "    elif effect == 1:\n",
    "        freq = random.randint(9,15)\n",
    "        decay = round(random.uniform(0.1,0.2), 2)\n",
    "        effect = \"vibrato=f={}:d={}\".format(freq, decay)\n",
    "    return str(effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7ae318f",
   "metadata": {
    "papermill": {
     "duration": 3.148945,
     "end_time": "2023-08-09T15:45:32.446746",
     "exception": false,
     "start_time": "2023-08-09T15:45:29.297801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math, random\n",
    "from numpy import random\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from torchaudio.io import AudioEffector\n",
    "from IPython.display import Audio\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class AudioUtil():\n",
    "    #---------------\n",
    "    # Load an audio file. Return the signal as a tensor and the sample rate\n",
    "    #---------------\n",
    "    @staticmethod\n",
    "    def open(audio_file):\n",
    "        sig, sr = torchaudio.load(audio_file)\n",
    "        #sig, sr = torchaudio.load(audio_file)\n",
    "        \n",
    "        return (sig, sr)\n",
    "    \n",
    "    #---------------\n",
    "    # Convert soundfile to desired number of channels\n",
    "    #---------------\n",
    "    @staticmethod\n",
    "    def rechannel(aud, new_channel):\n",
    "        \n",
    "        sig, sr = aud\n",
    "        \n",
    "        if sig.shape[0] == new_channel:\n",
    "            #Nothing todo\n",
    "            return aud\n",
    "        \n",
    "        if (new_channel == 1):\n",
    "            #Convert stereo to mono by selecting only the first channel\n",
    "            resig = sig[:1, :]\n",
    "        else:\n",
    "            #Convert from mono to sterio by duplicating the first channel\n",
    "            resig = torch.cat([sig,sig])\n",
    "        return ((resig, sr))\n",
    "    \n",
    "    #---------------\n",
    "    #Resample to make sure samplerate is the same for all files - resample applies to one channel at a time\n",
    "    #---------------\n",
    "    @staticmethod\n",
    "    def resample(aud, newsr):\n",
    "        \n",
    "        sig, sr = aud\n",
    "        \n",
    "        if (sr == newsr):\n",
    "            #do nothing\n",
    "            return aud\n",
    "        \n",
    "        num_channels = sig.shape[0]\n",
    "        \n",
    "        #resample first channel\n",
    "        resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n",
    "        \n",
    "        if (num_channels > 1):\n",
    "            #Resample the second channel and merge both\n",
    "            retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n",
    "            resig = torch.cat([resig, retwo])\n",
    "            \n",
    "        return ((resig, newsr))\n",
    "    \n",
    "    \n",
    "    #-----------------\n",
    "    #Pad or turncate the signal to be off a standard length in milliseconds\n",
    "    #-----------------\n",
    "    @staticmethod\n",
    "    def pad_trunc(aud, max_ms):\n",
    "\n",
    "        sig, sr = aud\n",
    "        num_rows, sig_len = sig.shape\n",
    "        max_len = sr//1000 * max_ms\n",
    "        \n",
    "        if (sig_len > max_len):\n",
    "            #Turncate the signal to the given length\n",
    "            sig = sig[:,:max_len]\n",
    "        elif (sig_len < max_len):\n",
    "            pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "            pad_end_len = max_len - sig_len - pad_begin_len\n",
    "            \n",
    "            #pad with zeroes\n",
    "            pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "            pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "            \n",
    "            sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "        \n",
    "        return (sig, sr)\n",
    "\n",
    "    #--------------------\n",
    "    #Shift the signal by a random bit, end of signal is wrapped around \n",
    "    #to beginning\n",
    "    #--------------------\n",
    "    @staticmethod\n",
    "    def time_shift(aud, shift_limit):\n",
    "        sig, sr = aud\n",
    "        \n",
    "        _, sig_len = sig.shape\n",
    "        shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "        return (sig.roll(shift_amt), sr)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def add_rand_effect(aud):\n",
    "        sig, sr = aud    \n",
    "        effect = random.randint(0,1)\n",
    "        if effect ==0:\n",
    "            delay = random.randint(40,100)\n",
    "            decay = round(random.uniform(0.2,0.7), 2)\n",
    "            effect = \"aecho=in_gain=0.8:out_gain=0.9:delays={}:decays={}\".format(delay, decay)\n",
    "        elif effect == 1:\n",
    "            freq = random.randint(9,15)\n",
    "            decay = round(random.uniform(0.15,0.4), 2)\n",
    "            effect = \"vibrato=f={}:d={}\".format(freq, decay)\n",
    "        effector = AudioEffector(effect=effect, pad_end=False)\n",
    "        sig_ef = effector.apply(sig.T, int(sr)) # int(sr)\n",
    "        sig_ef = sig_ef.T\n",
    "\n",
    "        return (sig_ef, sr)\n",
    "    \n",
    "    #----------------------------------\n",
    "    #Genetate spectrogram\n",
    "    #----------------------------------\n",
    "    @staticmethod\n",
    "    def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
    "        sig,sr = aud\n",
    "        top_db = 80\n",
    "        \n",
    "        #spec has shape [channel, n_mels, time]\n",
    "        spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "        \n",
    "        #convert to db\n",
    "        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "        \n",
    "        return spec\n",
    "    \n",
    "    \n",
    "    #Augment the spectrogram by masking out some sections of it in both the frequency\n",
    "    #dimencion (Horizontal) and the time dimension (vertical bars)\n",
    "    \n",
    "    @staticmethod\n",
    "    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "        _, n_mels, n_steps = spec.shape\n",
    "        mask_value = spec.mean()\n",
    "        aug_spec = spec\n",
    "        \n",
    "        freq_mask_param = max_mask_pct * n_mels\n",
    "        for _ in range(n_freq_masks):\n",
    "            aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "        \n",
    "        time_mask_param = max_mask_pct * n_steps\n",
    "        for _ in range(n_time_masks):\n",
    "            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "        \n",
    "        return np.absolute(aug_spec) # Jonas added np.absolute\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d1799d7",
   "metadata": {
    "papermill": {
     "duration": 0.020522,
     "end_time": "2023-08-09T15:45:32.472845",
     "exception": false,
     "start_time": "2023-08-09T15:45:32.452323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchaudio\n",
    "\n",
    "#----------------\n",
    "#Sound dataset\n",
    "#----------------\n",
    "    \n",
    "class SoundDS(Dataset):\n",
    "    def __init__(self, df, data_paths):\n",
    "        self.df = df\n",
    "        self.data_paths = data_paths\n",
    "        self.duration = 4000\n",
    "        self.sr = 44100\n",
    "        self.channel = 2\n",
    "        self.shift_pct = 0.4\n",
    "    \n",
    "    #------------------\n",
    "    #Number of items in dataset\n",
    "    #------------------\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    #------------------\n",
    "    #Get i'th item in dataset\n",
    "    #------------------\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Get the class ID  \n",
    "        class_id = self.df.loc[idx, 'classID']\n",
    "        audio_file = str(self.df.loc[idx, 'relative_path'])\n",
    "        aud = AudioUtil.open(audio_file)\n",
    "        if self.df.loc[idx, 'synth'] == True:\n",
    "            aud = AudioUtil.add_rand_effect(aud)\n",
    "        reaud = AudioUtil.resample(aud, self.sr)\n",
    "        rechan = AudioUtil.rechannel(reaud, self.channel)\n",
    "        #if self.df.loc[idx, 'synth'] == True:\n",
    "        #    rechan = AudioUtil.add_rand_effect(rechan)\n",
    "        dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n",
    "        shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n",
    "        sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
    "        aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "        return aug_sgram, class_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47e5e43d",
   "metadata": {
    "papermill": {
     "duration": 0.025648,
     "end_time": "2023-08-09T15:45:32.503499",
     "exception": false,
     "start_time": "2023-08-09T15:45:32.477851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Splitt data for training and validation\n",
    "\n",
    "myds = SoundDS(df, data_paths) #returns spectrogram and classID\n",
    "#Random split between training and validation data\n",
    "\n",
    "num_items = len(myds)\n",
    "num_train = round(num_items*0.8)\n",
    "num_val = num_items - num_train\n",
    "train_ds, val_ds = random_split(myds, [num_train, num_val])\n",
    "\n",
    "# Create training and validation data loaders\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22fb2f0e",
   "metadata": {
    "papermill": {
     "duration": 2.858849,
     "end_time": "2023-08-09T15:45:35.367495",
     "exception": false,
     "start_time": "2023-08-09T15:45:32.508646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from torch import nn\n",
    "\n",
    "#---------------------------\n",
    "# Audio Classification Model\n",
    "#---------------------------\n",
    "class AudioClassifier (nn.Module):\n",
    "    #---------------------------\n",
    "    #Build the model architecture\n",
    "    #---------------------------\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "        \n",
    "        #First convolution block with Relu and Batch Norm. Use Kaiming Initialisation\n",
    "        self.conv1 = nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1, self.bn1]\n",
    "\n",
    "        #Second convolution block\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        conv_layers += [self.conv2, self.relu2, self.bn2]\n",
    "        \n",
    "        #Third convolution block\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
    "\n",
    "        #Fourth convolution block\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        conv_layers += [self.conv4, self.relu4, self.bn4] \n",
    "        \n",
    "        # Linear classifier\n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.lin = nn.Linear(in_features=64, out_features=df.classID.max()+1) #Changed Jonas out_features\n",
    "        \n",
    "        # Wrap the convolutional blocks\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    "    \n",
    "    #-------------------------\n",
    "    #Forward pass computations\n",
    "    #-------------------------\n",
    "    def forward(self, x):\n",
    "        # Run convolutional blocks\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        # Adaptive pool and flatten for input to linear layer\n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        # Linear layer\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        # Final output\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create the model and put it on the GPU if available\n",
    "myModel =  AudioClassifier()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "myModel = myModel.to(device)\n",
    "#Check that it is on Cuda\n",
    "next(myModel.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4a98849",
   "metadata": {
    "papermill": {
     "duration": 420.38617,
     "end_time": "2023-08-09T15:52:35.759057",
     "exception": false,
     "start_time": "2023-08-09T15:45:35.372887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1.96, Accuracy: 0.34\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "#-------------------\n",
    "#Training loop\n",
    "#-------------------\n",
    "def training(model, train_dl, num_epochs):\n",
    "    #Loss function optimiser and Scheduler\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimiser, max_lr=0.001,\n",
    "                                                    steps_per_epoch=int(len(train_dl)),\n",
    "                                                    epochs=num_epochs,\n",
    "                                                    anneal_strategy='linear')\n",
    "    \n",
    "    #Repeat for each epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_prediction = 0\n",
    "        total_prediction = 0\n",
    "        \n",
    "        #repeat for each batch in the training set\n",
    "        for i, data in enumerate(train_dl):\n",
    "            # Get the input features and target labels and put them on the GPU\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            \n",
    "            #Normalise the inputs\n",
    "            inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "            inputs = (inputs - inputs_m) / inputs_s\n",
    "            \n",
    "            #Zero prameter gradients\n",
    "            optimiser.zero_grad()\n",
    "            \n",
    "            #Forward + backward + optimise\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            #Keep stats for Loss and Accuracy\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            #Get predicted class with highest score\n",
    "            _, prediction = torch.max(outputs, 1)\n",
    "            # Count the predictions that matched the target label\n",
    "            correct_prediction += (prediction == labels).sum().item()\n",
    "            total_prediction += prediction.shape[0]\n",
    "            \n",
    "            #if i % 10 == 0: #\n",
    "            #    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n",
    "        \n",
    "        #Print stats at the end of each epoch\n",
    "        num_batches = len(train_dl)\n",
    "        avg_loss = running_loss / num_batches\n",
    "        acc = correct_prediction / total_prediction\n",
    "        print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n",
    "    \n",
    "    print('Finished Training')\n",
    "    \n",
    "num_epochs = 1\n",
    "training(myModel, train_dl, num_epochs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43e1b638",
   "metadata": {
    "papermill": {
     "duration": 0.020647,
     "end_time": "2023-08-09T15:52:35.784741",
     "exception": false,
     "start_time": "2023-08-09T15:52:35.764094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#-------------------\n",
    "#Inferrence\n",
    "#-------------------\n",
    "\n",
    "def inferrence (model, v_dl):\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "    \n",
    "    classIDs = sorted(list(df.classID.unique()))\n",
    "    df_metr = pd.DataFrame({'classIDs': classIDs,\n",
    "                        'correct_pred': np.zeros(len(classIDs)),\n",
    "                        'false_pred': np.zeros(len(classIDs)),\n",
    "                        'accuracy': np.zeros(len(classIDs))})\n",
    "    df_metr = df_metr.set_index('classIDs')\n",
    "    \n",
    "    #Disable gradient updates\n",
    "    with torch.no_grad():\n",
    "        for data in v_dl:\n",
    "            # Get the input features and target labels and put them on the GPU\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            #Normalise the inputs\n",
    "            inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "            inputs = (inputs - inputs_m) / inputs_s\n",
    "            \n",
    "            #Get predictions\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            #Get predicted putputs with highest score\n",
    "            i, prediction = torch.max(outputs,1)\n",
    "            for n in range(len(outputs)):\n",
    "                if labels[n].item() == -1:\n",
    "                    print(\"Predicted class:{}, score:{}, actual class:{}\".format(prediction[n].item(), outputs[n].max().item(), labels[n].item()))\n",
    "                else:\n",
    "                    if prediction[n].item() == labels[n].item():\n",
    "                        df_metr['correct_pred'].loc[prediction[n].item()] += 1\n",
    "                    else:\n",
    "                        df_metr['false_pred'].loc[prediction[n].item()] += 1\n",
    "                #print(\"Predicted class:{}, score:{}, actual class:{}\".format(prediction[n].item(), outputs[n].max().item(), labels[n].item()))\n",
    "            #print(outputs[15].max(),prediction[15], labels[15])\n",
    "            #Count of predictions that matched target label\n",
    "            correct_prediction += (prediction == labels).sum().item()\n",
    "            total_prediction += prediction.shape[0]\n",
    "            \n",
    "    acc = correct_prediction / total_prediction\n",
    "    df_metr['accuracy'] = df_metr['correct_pred'] / (df_metr['correct_pred'] + df_metr['false_pred'])\n",
    "    print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')\n",
    "    return df_metr\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#Run inferrence on trained model with validation set\n",
    "#inferrence(myModel, val_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed16cfee",
   "metadata": {
    "papermill": {
     "duration": 65.253264,
     "end_time": "2023-08-09T15:53:41.043234",
     "exception": false,
     "start_time": "2023-08-09T15:52:35.789970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.44, Total items: 2038\n"
     ]
    }
   ],
   "source": [
    "d = inferrence(myModel, val_dl)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "314c94fb",
   "metadata": {
    "papermill": {
     "duration": 0.01937,
     "end_time": "2023-08-09T15:53:41.067792",
     "exception": false,
     "start_time": "2023-08-09T15:53:41.048422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          correct_pred  false_pred  accuracy\n",
      "classIDs                                    \n",
      "0                 65.0       155.0  0.295455\n",
      "1                 18.0        56.0  0.243243\n",
      "2                 67.0       122.0  0.354497\n",
      "3                 89.0       168.0  0.346304\n",
      "4                 45.0        49.0  0.478723\n",
      "5                 74.0       124.0  0.373737\n",
      "6                 20.0        21.0  0.487805\n",
      "7                110.0       143.0  0.434783\n",
      "8                 57.0        73.0  0.438462\n",
      "9                 92.0       184.0  0.333333\n",
      "10                89.0        14.0  0.864078\n",
      "11               119.0        24.0  0.832168\n",
      "12                59.0         1.0  0.983333\n"
     ]
    }
   ],
   "source": [
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cbbe592",
   "metadata": {
    "papermill": {
     "duration": 0.021362,
     "end_time": "2023-08-09T15:53:41.094845",
     "exception": false,
     "start_time": "2023-08-09T15:53:41.073483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "meta3 = str(data_paths['appliances']) + '/test/' + 'test.csv'\n",
    "df_test = pd.read_csv(meta3)\n",
    "#df_test = df_test.sample(frac=150, replace=True, random_state=1)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "df_test['relative_path'] = 'appliances/' + df_test['fold'].astype(str) + '/' + df_test['slice_file_name'].astype(str)\n",
    "df_test['synth'] = False\n",
    "df_test = df_test[['relative_path', 'classID', 'synth']]\n",
    "\n",
    "#df_test2 = \n",
    "\n",
    "#Onedrive_postdoc/OneDrive - University of Exeter/sound_recognition/appliances/test/9000028-11-0-0.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0529e525-3cb2-465e-a62a-59f9e6f430cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        relative_path  classID  synth\n",
      "0  appliances/test/9000000-11-0-0.wav       11  False\n",
      "1  appliances/test/9000003-11-0-0.wav       11  False\n",
      "2  appliances/test/9000009-11-0-0.wav       11  False\n",
      "3  appliances/test/9000018-11-0-0.wav       11  False\n",
      "4  appliances/test/9000019-11-0-0.wav       11  False\n",
      "5  appliances/test/9000022-11-0-0.wav       11  False\n",
      "6  appliances/test/9000028-11-0-0.wav       11  False\n",
      "7  appliances/test/9000044-12-0-0.wav       10  False\n"
     ]
    }
   ],
   "source": [
    "print(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3393d349",
   "metadata": {
    "papermill": {
     "duration": 0.013961,
     "end_time": "2023-08-09T15:53:41.114434",
     "exception": false,
     "start_time": "2023-08-09T15:53:41.100473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_test = SoundDS(df_test, data_paths)\n",
    "test_dl = torch.utils.data.DataLoader(ds_test, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0cdaead8",
   "metadata": {
    "papermill": {
     "duration": 11.660271,
     "end_time": "2023-08-09T15:53:52.780028",
     "exception": false,
     "start_time": "2023-08-09T15:53:41.119757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.12, Total items: 8\n",
      "          correct_pred  false_pred  accuracy\n",
      "classIDs                                    \n",
      "0                  0.0         0.0       NaN\n",
      "1                  0.0         0.0       NaN\n",
      "2                  0.0         0.0       NaN\n",
      "3                  0.0         2.0       0.0\n",
      "4                  0.0         0.0       NaN\n",
      "5                  0.0         3.0       0.0\n",
      "6                  0.0         0.0       NaN\n",
      "7                  0.0         0.0       NaN\n",
      "8                  0.0         0.0       NaN\n",
      "9                  0.0         2.0       0.0\n",
      "10                 1.0         0.0       1.0\n",
      "11                 0.0         0.0       NaN\n",
      "12                 0.0         0.0       NaN\n"
     ]
    }
   ],
   "source": [
    "d = inferrence(myModel, test_dl)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94dfb15d-3e05-4397-92c9-d1e9fe2deeb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'audio_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m aud \u001b[38;5;241m=\u001b[39m AudioUtil\u001b[38;5;241m.\u001b[39mopen(\u001b[43maudio_file\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'audio_file' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "aud = AudioUtil.open(audio_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 515.839578,
   "end_time": "2023-08-09T15:53:55.210633",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-09T15:45:19.371055",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
