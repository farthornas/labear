{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "5806326a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.101861,
     "end_time": "2023-08-07T13:20:22.013170",
     "exception": false,
     "start_time": "2023-08-07T13:20:21.911309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relative_path</th>\n",
       "      <th>classID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/fold5/100032-3-0-0.wav</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/fold5/100263-2-0-117.wav</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/fold5/100263-2-0-121.wav</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/fold5/100263-2-0-126.wav</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/fold5/100263-2-0-137.wav</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               relative_path  classID\n",
       "0    /fold5/100032-3-0-0.wav        3\n",
       "1  /fold5/100263-2-0-117.wav        2\n",
       "2  /fold5/100263-2-0-121.wav        2\n",
       "3  /fold5/100263-2-0-126.wav        2\n",
       "4  /fold5/100263-2-0-137.wav        2"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "dir_path = ''\n",
    "kaggle = '/kaggle/input/'\n",
    "if os.path.isdir(kaggle):\n",
    "    dir_path = kaggle\n",
    "# Read metadata file\n",
    "data_paths = {'appliances':dir_path + 'appliances',\n",
    "              'urban':dir_path + 'urbansound8k'}\n",
    "meta1 = data_paths['appliances'] + '/' + 'appliances.csv'\n",
    "meta2 = data_paths['urban'] + '/' + 'UrbanSound8K.csv'\n",
    "df_UrbanSound = pd.read_csv(meta2)\n",
    "df_Appliences = pd.read_csv(meta1)\n",
    "smpl = df_UrbanSound.classID.value_counts().max() / df_Appliences.classID.value_counts().min()\n",
    "df_Appliences = df_Appliences.sample(frac=smpl, replace=True, random_state=1)\n",
    "#df_UrbanSound = df_UrbanSound.sample(frac=(1/smpl), replace=True, random_state=1)\n",
    "\n",
    "df_UrbanSound['relative_path'] = '/fold' + df_UrbanSound['fold'].astype(str) + '/' + df_UrbanSound['slice_file_name'].astype(str)\n",
    "df_Appliences['relative_path'] = '/' + df_Appliences['fold'].astype(str) + '/' + df_Appliences['slice_file_name'].astype(str)\n",
    "\n",
    "df = pd.concat([df_UrbanSound,df_Appliences])\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()\n",
    "\n",
    "#Take relative columns\n",
    "df = df[['relative_path', 'classID']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "57bdfbcb",
   "metadata": {
    "papermill": {
     "duration": 0.012692,
     "end_time": "2023-08-07T13:20:22.029838",
     "exception": false,
     "start_time": "2023-08-07T13:20:22.017146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11    1786\n",
      "10    1471\n",
      "12    1016\n",
      "3     1000\n",
      "2     1000\n",
      "0     1000\n",
      "9     1000\n",
      "5     1000\n",
      "7     1000\n",
      "4     1000\n",
      "8      929\n",
      "1      429\n",
      "6      374\n",
      "Name: classID, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.classID.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "7192bc97",
   "metadata": {
    "papermill": {
     "duration": 3.599959,
     "end_time": "2023-08-07T13:20:25.633552",
     "exception": false,
     "start_time": "2023-08-07T13:20:22.033593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math, random\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from IPython.display import Audio\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class AudioUtil():\n",
    "    #---------------\n",
    "    # Load an audio file. Return the signal as a tensor and the sample rate\n",
    "    #---------------\n",
    "    @staticmethod\n",
    "    def open(audio_file):\n",
    "        sig, sr = torchaudio.load(audio_file)\n",
    "        return (sig, sr)\n",
    "    \n",
    "    #---------------\n",
    "    # Convert soundfile to desired number of channels\n",
    "    #---------------\n",
    "    @staticmethod\n",
    "    def rechannel(aud, new_channel):\n",
    "        \n",
    "        sig, sr = aud\n",
    "        \n",
    "        if sig.shape[0] == new_channel:\n",
    "            #Nothing todo\n",
    "            return aud\n",
    "        \n",
    "        if (new_channel == 1):\n",
    "            #Convert stereo to mono by selecting only the first channel\n",
    "            resig = sig[:1, :]\n",
    "        else:\n",
    "            #Convert from mono to sterio by duplicating the first channel\n",
    "            resig = torch.cat([sig,sig])\n",
    "        return ((resig, sr))\n",
    "    \n",
    "    #---------------\n",
    "    #Resample to make sure samplerate is the same for all files - resample applies to one channel at a time\n",
    "    #---------------\n",
    "    @staticmethod\n",
    "    def resample(aud, newsr):\n",
    "        \n",
    "        sig, sr = aud\n",
    "        \n",
    "        if (sr == newsr):\n",
    "            #do nothing\n",
    "            return aud\n",
    "        \n",
    "        num_channels = sig.shape[0]\n",
    "        \n",
    "        #resample first channel\n",
    "        resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n",
    "        \n",
    "        if (num_channels > 1):\n",
    "            #Resample the second channel and merge both\n",
    "            retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n",
    "            resig = torch.cat([resig, retwo])\n",
    "            \n",
    "        return ((resig, newsr))\n",
    "    \n",
    "    \n",
    "    #-----------------\n",
    "    #Pad or turncate the signal to be off a standard length in milliseconds\n",
    "    #-----------------\n",
    "    @staticmethod\n",
    "    def pad_trunc(aud, max_ms):\n",
    "\n",
    "        sig, sr = aud\n",
    "        num_rows, sig_len = sig.shape\n",
    "        max_len = sr//1000 * max_ms\n",
    "        \n",
    "        if (sig_len > max_len):\n",
    "            #Turncate the signal to the given length\n",
    "            sig = sig[:,:max_len]\n",
    "        elif (sig_len < max_len):\n",
    "            pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "            pad_end_len = max_len - sig_len - pad_begin_len\n",
    "            \n",
    "            #pad with zeroes\n",
    "            pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "            pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "            \n",
    "            sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "        \n",
    "        return (sig, sr)\n",
    "\n",
    "    #--------------------\n",
    "    #Shift the signal by a random bit, end of signal is wrapped around \n",
    "    #to beginning\n",
    "    #--------------------\n",
    "    @staticmethod\n",
    "    def time_shift(aud, shift_limit):\n",
    "        sig, sr = aud\n",
    "        \n",
    "        _, sig_len = sig.shape\n",
    "        shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "        return (sig.roll(shift_amt), sr)\n",
    "    \n",
    "    #----------------------------------\n",
    "    #Genetate spectrogram\n",
    "    #----------------------------------\n",
    "    @staticmethod\n",
    "    def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
    "        sig,sr = aud\n",
    "        top_db = 80\n",
    "        \n",
    "        #spec has shape [channel, n_mels, time]\n",
    "        spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "        \n",
    "        #convert to db\n",
    "        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "        \n",
    "        return spec\n",
    "    \n",
    "    \n",
    "    #Augment the spectrogram by masking out some sections of it in both the frequency\n",
    "    #dimencion (Horizontal) and the time dimension (vertical bars)\n",
    "    \n",
    "    @staticmethod\n",
    "    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "        _, n_mels, n_steps = spec.shape\n",
    "        mask_value = spec.mean()\n",
    "        aug_spec = spec\n",
    "        \n",
    "        freq_mask_param = max_mask_pct * n_mels\n",
    "        for _ in range(n_freq_masks):\n",
    "            aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "        \n",
    "        time_mask_param = max_mask_pct * n_steps\n",
    "        for _ in range(n_time_masks):\n",
    "            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "        \n",
    "        return np.absolute(aug_spec) # Jonas added np.absolute\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "edca83ed",
   "metadata": {
    "papermill": {
     "duration": 0.017729,
     "end_time": "2023-08-07T13:20:25.655268",
     "exception": false,
     "start_time": "2023-08-07T13:20:25.637539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchaudio\n",
    "\n",
    "#----------------\n",
    "#Sound dataset\n",
    "#----------------\n",
    "    \n",
    "class SoundDS(Dataset):\n",
    "    def __init__(self, df, data_paths):\n",
    "        self.df = df\n",
    "        self.data_paths = data_paths\n",
    "        self.duration = 4000\n",
    "        self.sr = 44100\n",
    "        self.channel = 2\n",
    "        self.shift_pct = 0.4\n",
    "    \n",
    "    #------------------\n",
    "    #Number of items in dataset\n",
    "    #------------------\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    #------------------\n",
    "    #Get i'th item in dataset\n",
    "    #------------------\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Get the class ID  \n",
    "        class_id = self.df.loc[idx, 'classID']\n",
    "        # Absolute file path of the audio file - concatenate the audio direcory\n",
    "        # with the relative path\n",
    "        if class_id <= df_UrbanSound['classID'].max():  # UrbanSound data set (classIDs 0 - 10)\n",
    "            audio_file = self.data_paths['urban'] + self.df.loc[idx, 'relative_path']\n",
    "        else: # Appliences (Class IDs => 10)\n",
    "            audio_file = self.data_paths['appliances'] + self.df.loc[idx, 'relative_path']\n",
    "              \n",
    "        aud = AudioUtil.open(audio_file)\n",
    "        #Make all the sounds have the same number of channels and same sample rate\n",
    "        #Then make all samples the same length\n",
    "        reaud = AudioUtil.resample(aud, self.sr)\n",
    "        rechan = AudioUtil.rechannel(reaud, self.channel)\n",
    "        \n",
    "        dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n",
    "        shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n",
    "        sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
    "        aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "        return aug_sgram, class_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "331b4253",
   "metadata": {
    "papermill": {
     "duration": 0.022772,
     "end_time": "2023-08-07T13:20:25.681870",
     "exception": false,
     "start_time": "2023-08-07T13:20:25.659098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Splitt data for training and validation\n",
    "\n",
    "myds = SoundDS(df, data_paths) #returns spectrogram and classID\n",
    "#Random split between training and validation data\n",
    "\n",
    "num_items = len(myds)\n",
    "num_train = round(num_items*0.8)\n",
    "num_val = num_items - num_train\n",
    "train_ds, val_ds = random_split(myds, [num_train, num_val])\n",
    "\n",
    "# Create training and validation data loaders\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "6258e4a6",
   "metadata": {
    "papermill": {
     "duration": 0.062169,
     "end_time": "2023-08-07T13:20:25.748015",
     "exception": false,
     "start_time": "2023-08-07T13:20:25.685846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from torch import nn\n",
    "\n",
    "#---------------------------\n",
    "# Audio Classification Model\n",
    "#---------------------------\n",
    "class AudioClassifier (nn.Module):\n",
    "    #---------------------------\n",
    "    #Build the model architecture\n",
    "    #---------------------------\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "        \n",
    "        #First convolution block with Relu and Batch Norm. Use Kaiming Initialisation\n",
    "        self.conv1 = nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1, self.bn1]\n",
    "\n",
    "        #Second convolution block\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        conv_layers += [self.conv2, self.relu2, self.bn2]\n",
    "        \n",
    "        #Third convolution block\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
    "\n",
    "        #Fourth convolution block\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        conv_layers += [self.conv4, self.relu4, self.bn4] \n",
    "        \n",
    "        # Linear classifier\n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.lin = nn.Linear(in_features=64, out_features=df.classID.max()+1) #Changed Jonas out_features\n",
    "        \n",
    "        # Wrap the convolutional blocks\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    "    \n",
    "    #-------------------------\n",
    "    #Forward pass computations\n",
    "    #-------------------------\n",
    "    def forward(self, x):\n",
    "        # Run convolutional blocks\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        # Adaptive pool and flatten for input to linear layer\n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        # Linear layer\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        # Final output\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create the model and put it on the GPU if available\n",
    "myModel =  AudioClassifier()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "myModel = myModel.to(device)\n",
    "#Check that it is on Cuda\n",
    "next(myModel.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "702c3f6f",
   "metadata": {
    "papermill": {
     "duration": 5.405096,
     "end_time": "2023-08-07T13:20:31.157154",
     "exception": false,
     "start_time": "2023-08-07T13:20:25.752058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1.73, Accuracy: 0.43\n",
      "Epoch: 1, Loss: 1.23, Accuracy: 0.57\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "#-------------------\n",
    "#Training loop\n",
    "#-------------------\n",
    "def training(model, train_dl, num_epochs):\n",
    "    #Loss function optimiser and Scheduler\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimiser, max_lr=0.001,\n",
    "                                                    steps_per_epoch=int(len(train_dl)),\n",
    "                                                    epochs=num_epochs,\n",
    "                                                    anneal_strategy='linear')\n",
    "    \n",
    "    #Repeat for each epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_prediction = 0\n",
    "        total_prediction = 0\n",
    "        \n",
    "        #repeat for each batch in the training set\n",
    "        for i, data in enumerate(train_dl):\n",
    "            # Get the input features and target labels and put them on the GPU\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            \n",
    "            #Normalise the inputs\n",
    "            inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "            inputs = (inputs - inputs_m) / inputs_s\n",
    "            \n",
    "            #Zero prameter gradients\n",
    "            optimiser.zero_grad()\n",
    "            \n",
    "            #Forward + backward + optimise\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            #Keep stats for Loss and Accuracy\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            #Get predicted class with highest score\n",
    "            _, prediction = torch.max(outputs, 1)\n",
    "            # Count the predictions that matched the target label\n",
    "            correct_prediction += (prediction == labels).sum().item()\n",
    "            total_prediction += prediction.shape[0]\n",
    "            \n",
    "            #if i % 10 == 0: #\n",
    "            #    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n",
    "        \n",
    "        #Print stats at the end of each epoch\n",
    "        num_batches = len(train_dl)\n",
    "        avg_loss = running_loss / num_batches\n",
    "        acc = correct_prediction / total_prediction\n",
    "        print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n",
    "    \n",
    "    print('Finished Training')\n",
    "    \n",
    "num_epochs = 2\n",
    "training(myModel, train_dl, num_epochs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "cf3e7324",
   "metadata": {
    "papermill": {
     "duration": 0.732595,
     "end_time": "2023-08-07T13:20:31.894096",
     "exception": false,
     "start_time": "2023-08-07T13:20:31.161501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#-------------------\n",
    "#Inferrence\n",
    "#-------------------\n",
    "\n",
    "def inferrence (model, v_dl):\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "    \n",
    "    #classIDs = sorted(list(len(df.classID.max())))\n",
    "    df_metr = pd.DataFrame({'classIDs': [0,1,2,3,4,5,6,7,8,9,10,11,12],\n",
    "                        'correct_pred': np.zeros(len(classIDs)),\n",
    "                        'false_pred': np.zeros(len(classIDs)),\n",
    "                        'accuracy': np.zeros(len(classIDs))})\n",
    "    df_metr = df_metr.set_index('classIDs')\n",
    "    \n",
    "    #Disable gradient updates\n",
    "    with torch.no_grad():\n",
    "        for data in v_dl:\n",
    "            # Get the input features and target labels and put them on the GPU\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            #Normalise the inputs\n",
    "            inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "            inputs = (inputs - inputs_m) / inputs_s\n",
    "            \n",
    "            #Get predictions\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            #Get predicted putputs with highest score\n",
    "            i, prediction = torch.max(outputs,1)\n",
    "            for n in range(len(outputs)):\n",
    "                if labels[n].item() == -1:\n",
    "                    print(\"Predicted class:{}, score:{}, actual class:{}\".format(prediction[n].item(), outputs[n].max().item(), labels[n].item()))\n",
    "                else:\n",
    "                    if prediction[n].item() == labels[n].item():\n",
    "                        df_metr['correct_pred'].loc[prediction[n].item()] += 1\n",
    "                    else:\n",
    "                        df_metr['false_pred'].loc[prediction[n].item()] += 1\n",
    "                #print(\"Predicted class:{}, score:{}, actual class:{}\".format(prediction[n].item(), outputs[n].max().item(), labels[n].item()))\n",
    "            #print(outputs[15].max(),prediction[15], labels[15])\n",
    "            #Count of predictions that matched target label\n",
    "            correct_prediction += (prediction == labels).sum().item()\n",
    "            total_prediction += prediction.shape[0]\n",
    "            \n",
    "    acc = correct_prediction / total_prediction\n",
    "    df_metr['accuracy'] = df_metr['correct_pred'] / (df_metr['correct_pred'] + df_metr['false_pred'])\n",
    "    print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')\n",
    "    return df_metr\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#Run inferrence on trained model with validation set\n",
    "#inferrence(myModel, val_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "58304f57-93ee-45d3-bec1-83fa4baf1122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.61, Total items: 2601\n"
     ]
    }
   ],
   "source": [
    "d = inferrence(myModel, val_dl)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "8cab4267-fcfe-4959-b15d-6aef1ebfb8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          correct_pred  false_pred  accuracy\n",
      "classIDs                                    \n",
      "0                 70.0       107.0  0.395480\n",
      "1                 20.0        36.0  0.357143\n",
      "2                 86.0       146.0  0.370690\n",
      "3                 81.0       108.0  0.428571\n",
      "4                 70.0        52.0  0.573770\n",
      "5                 96.0       118.0  0.448598\n",
      "6                 36.0        29.0  0.553846\n",
      "7                 95.0       120.0  0.441860\n",
      "8                 89.0       138.0  0.392070\n",
      "9                 82.0       112.0  0.422680\n",
      "10               321.0        15.0  0.955357\n",
      "11               337.0        24.0  0.933518\n",
      "12               206.0         7.0  0.967136\n"
     ]
    }
   ],
   "source": [
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "7dcbe715-6683-48d0-b285-7688fdc6febe",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta3 = 'appliances/test.csv'\n",
    "df_test = pd.read_csv(meta3)\n",
    "df_test = df_test.sample(frac=150, replace=True, random_state=1)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "df_test['relative_path'] = '/' + df_test['fold'].astype(str) + '/' + df_test['slice_file_name'].astype(str)\n",
    "df_test = df_test[['relative_path', 'classID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "2c75065b-d7ba-44b3-a4d1-39f2b612554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = SoundDS(df_test, data_paths)\n",
    "test_dl = torch.utils.data.DataLoader(ds_test, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "d5480017-d4b6-4e70-98e5-9793a390b7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.10, Total items: 1050\n",
      "          correct_pred  false_pred  accuracy\n",
      "classIDs                                    \n",
      "0                  0.0       108.0       0.0\n",
      "1                  0.0        15.0       0.0\n",
      "2                  0.0       141.0       0.0\n",
      "3                  0.0        60.0       0.0\n",
      "4                  0.0        20.0       0.0\n",
      "5                  0.0        75.0       0.0\n",
      "6                  0.0        62.0       0.0\n",
      "7                  0.0        54.0       0.0\n",
      "8                  0.0       149.0       0.0\n",
      "9                  0.0       153.0       0.0\n",
      "10                 0.0        72.0       0.0\n",
      "11               103.0         0.0       1.0\n",
      "12                 0.0        38.0       0.0\n"
     ]
    }
   ],
   "source": [
    "d = inferrence(myModel, test_dl)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b21052-3c1f-46c6-88a1-2b3736f6400f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19.907069,
   "end_time": "2023-08-07T13:20:33.202864",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-07T13:20:13.295795",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
