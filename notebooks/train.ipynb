{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "518dbb12-4466-4405-acdd-97b0f88f0a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install speechbrain==1.0.0\n",
    "# !pip install torch torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "375f7164-40f9-466c-b68e-db27045335ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install librosa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "389eb787-435c-42fc-9a48-f0a46082ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "from pydub import AudioSegment\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import IPython.display as display\n",
    "from IPython.display import Audio\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "import torchaudio\n",
    "import librosa\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db74c881-3839-44fd-b947-191c98ba0fa5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Inspect training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "8d6180ac-441c-40bc-bef7-eb9837419cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = ['air', 'vac', 'off']\n",
    "files_path = \"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei\"\n",
    "#files_path2 = \"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28\"\n",
    "\n",
    "path = Path(files_path)\n",
    "#path2 = Path(files_path2)\n",
    "\n",
    "files = []\n",
    "for p in path.rglob(\"*.wav\"):\n",
    "    files.append(str(p))\n",
    "    #files += glob.glob(f\"../data/{cls}/*.wav\")\n",
    "#for p in path2.rglob(\"*.wav\"):\n",
    "#    files.append(str(p))\n",
    "#    #files += glob.glob(f\"../data/{cls}/*.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "6423dcd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_8.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_9.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_23.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_22.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_20.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_21.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_25.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_19.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_18.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_30.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_24.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_26.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_27.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_16.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_17.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_29.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_15.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_14.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_28.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_10.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_11.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_13.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_12.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_4.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_5.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_7.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_6.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_2.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_3.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_1.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_0.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/vac/vac_23.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/vac/vac_7.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/vac/vac_6.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/vac/vac_22.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/vac/vac_20.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/vac/vac_4.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/vac/vac_5.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/vac/vac_21.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/vac/vac_19.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/vac/vac_25.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/vac/vac_1.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/vac/vac_0.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/vac/vac_30.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/vac/vac_24.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/vac/vac_18.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/vac/vac_26.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/vac/vac_2.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/vac/vac_3.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/vac/vac_27.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/vac/vac_16.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/vac/vac_17.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/vac/vac_15.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/vac/vac_29.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/vac/vac_28.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/vac/vac_14.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/vac/vac_10.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/vac/vac_8.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/vac/vac_9.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/vac/vac_11.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/vac/vac_13.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/vac/vac_12.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/off/off_26.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/off/off_27.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/off/off_25.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/off/off_19.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/off/off_18.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/off/off_24.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/off/off_30.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/off/off_20.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/off/off_21.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/off/off_23.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/off/off_22.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/off/off_7.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/off/off_6.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/off/off_4.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/off/off_5.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/off/off_1.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/off/off_0.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/off/off_2.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/off/off_3.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/off/off_8.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/off/off_9.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/off/off_13.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/off/off_12.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/off/off_10.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/off/off_11.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/off/off_29.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/off/off_15.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/off/off_14.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/off/off_28.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/off/off_16.wav', '/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/off/off_17.wav']\n"
     ]
    }
   ],
   "source": [
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "id": "b8c6dd63-ec9a-486b-9aa0-ea862701f6ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>label</th>\n",
       "      <th>name</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>/Users/jonas/Library/CloudStorage/OneDrive-Uni...</td>\n",
       "      <td>air</td>\n",
       "      <td>air_16.wav</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>/Users/jonas/Library/CloudStorage/OneDrive-Uni...</td>\n",
       "      <td>air</td>\n",
       "      <td>air_12.wav</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/jonas/Library/CloudStorage/OneDrive-Uni...</td>\n",
       "      <td>air</td>\n",
       "      <td>air_8.wav</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>/Users/jonas/Library/CloudStorage/OneDrive-Uni...</td>\n",
       "      <td>off</td>\n",
       "      <td>off_7.wav</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>/Users/jonas/Library/CloudStorage/OneDrive-Uni...</td>\n",
       "      <td>off</td>\n",
       "      <td>off_1.wav</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             filepath label        name  \\\n",
       "13  /Users/jonas/Library/CloudStorage/OneDrive-Uni...   air  air_16.wav   \n",
       "22  /Users/jonas/Library/CloudStorage/OneDrive-Uni...   air  air_12.wav   \n",
       "0   /Users/jonas/Library/CloudStorage/OneDrive-Uni...   air   air_8.wav   \n",
       "73  /Users/jonas/Library/CloudStorage/OneDrive-Uni...   off   off_7.wav   \n",
       "77  /Users/jonas/Library/CloudStorage/OneDrive-Uni...   off   off_1.wav   \n",
       "\n",
       "    duration  \n",
       "13       4.0  \n",
       "22       4.0  \n",
       "0        4.0  \n",
       "73       4.0  \n",
       "77       4.0  "
      ]
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dict(filepath=files))\n",
    "df[\"label\"] = df.filepath.apply(lambda x: Path(x).parent.name)\n",
    "df[\"name\"] = df.filepath.apply(lambda x: Path(x).name)\n",
    "df[\"duration\"] = df.filepath.apply(lambda x: librosa.get_duration(path=x))\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "id": "3e336f4e-6aca-4261-9070-ef8db382ed3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "duration\n",
       "4.0    90\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 676,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop samples that are to short in duration (< 3.5 secs)\n",
    "df = df.drop(df[(df.duration < 3.99)].index)\n",
    "df.duration.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "id": "e25783d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the first sample in all categories\n",
    "# as first sample often has silence in it\n",
    "#df_0 = df[df.name.str.contains(\"_0.wav\")]\n",
    "df = df[~df.name.str.contains(\"_0.wav\")]\n",
    "#df = df[~df.name.str.contains(\"_31.wav\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "edf66cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "air    29\n",
       "vac    29\n",
       "off    29\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 679,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "id": "42278b54-1381-4086-bbdc-153bd51b6515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_of(cls: str, random_state=None):\n",
    "    return df[df.label == cls].sample(random_state=random_state).iloc[0]\n",
    "    \n",
    "def play_sample(sample):\n",
    "    print(sample.filepath, sample.label)\n",
    "    return display.Audio(sample.filepath, autoplay=True)\n",
    "\n",
    "def play_sample_of(cls: str, random_state=None):\n",
    "    sample = get_sample_of(cls, random_state)\n",
    "    return play_sample(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "id": "67ad43ae-7a10-45ab-9d44-b2ed7b66834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(filepath, title=None):\n",
    "    y, sr = librosa.load(filepath)\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(S_dB, x_axis='time', y_axis='mel', sr=sr, fmax=8000)\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(f'Mel-frequency spectrogram ({title})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def show_sample(sample, **kwargs):\n",
    "    plot_spectrogram(sample.filepath, **kwargs)\n",
    "\n",
    "def show_sample_of(cls: str, random_state=None):\n",
    "    sample = get_sample_of(cls, random_state)\n",
    "    return show_sample(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1cb028-546a-405c-b183-414027ca634f",
   "metadata": {},
   "source": [
    "## Train/validation split\n",
    "\n",
    "Split data into training and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "id": "ccc5f4b0-5ef9-4560-a4d2-0687bde34dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "id": "2070c83a-ce72-4d72-ace4-7ef4a3a59a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((69, 4), (18, 4))"
      ]
     },
     "execution_count": 684,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, val = train_test_split(df, stratify=df.label, random_state=0, test_size=0.20)\n",
    "train.shape, val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "id": "77119351-a85d-4a42-82b0-ff349120b7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['isval'] = [i in val.index for i in df.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "id": "d317fff1-9507-4f0c-96bd-b35dd65c833b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isval\n",
       "False    69\n",
       "True     18\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 686,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isval.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "id": "a274f2d7-a99c-455d-9ced-3ddedee8ee21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "off    23\n",
       "air    23\n",
       "vac    23\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 687,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "id": "771d4e08-234c-4e2a-aece-0215b7292561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     air_22.wav\n",
       "65    off_19.wav\n",
       "13    air_16.wav\n",
       "16    air_15.wav\n",
       "79     off_2.wav\n",
       "82     off_9.wav\n",
       "67    off_24.wav\n",
       "91    off_16.wav\n",
       "36     vac_4.wav\n",
       "25     air_7.wav\n",
       "53    vac_29.wav\n",
       "22    air_12.wav\n",
       "0      air_8.wav\n",
       "54    vac_28.wav\n",
       "35    vac_20.wav\n",
       "37     vac_5.wav\n",
       "39    vac_19.wav\n",
       "62    off_26.wav\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea5e100-6db8-488c-baed-74efc6492845",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Import / download pretrained model (speechbrain urbansound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "id": "27ae1a98-1898-451b-b19d-f2174d986c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog_bark']\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "from speechbrain.inference.classifiers import EncoderClassifier\n",
    "classifier = EncoderClassifier.from_hparams(source=\"speechbrain/urbansound8k_ecapa\", savedir=\"models/gurbansound8k_ecapa\")\n",
    "out_prob, score, index, text_lab = classifier.classify_file('speechbrain/urbansound8k_ecapa/dog_bark.wav')\n",
    "print(text_lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c607d70",
   "metadata": {},
   "source": [
    "# Check that our files are compatible with the downloaded classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "id": "41c989ee-139c-4b58-a2d0-638700a0b35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.3198, -0.2624,  0.4991, -0.4318, -0.0067, -0.0238,  0.2409,  0.3789,\n",
       "           0.3183, -0.1127]]),\n",
       " tensor([0.4991]),\n",
       " tensor([2]),\n",
       " ['air_conditioner'])"
      ]
     },
     "execution_count": 690,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "rand = random.randint(0,(len(cats) - 1))\n",
    "sample = get_sample_of(cats[rand], random_state=1)\n",
    "classifier.classify_file(sample.filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348e937b-16c7-49f5-bc51-dbdeff6587ca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Convert audio to tensors for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "id": "2bb72294-59a9-47fb-bc4f-54fa3c2cff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "c301e06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_8.wav'"
      ]
     },
     "execution_count": 692,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0].filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7f4ee2-4675-4298-be99-d78dde51cce2",
   "metadata": {},
   "source": [
    "different samples have different length tensor (longer or shorter time series) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "c2712ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "import numpy as np\n",
    "from torchaudio import transforms\n",
    "from torchaudio.io import AudioEffector\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class AudioUtil():\n",
    "    #---------------\n",
    "    # Load an audio file. Return the signal as a tensor and the sample rate\n",
    "    #---------------\n",
    "    @staticmethod\n",
    "    def open(audio_file):\n",
    "        sig, sr = torchaudio.load(audio_file)\n",
    "        \n",
    "        return (sig, sr)\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_gain(aud, sig_max=0.9):\n",
    "        sig, sr = aud\n",
    "        gn = sig_max / sig.max()\n",
    "\n",
    "        transform = transforms.Vol(gain=gn, gain_type=\"amplitude\")\n",
    "        \n",
    "        sig_amp = transform(sig)\n",
    "        \n",
    "        return (sig_amp, sr)\n",
    "    \n",
    "    #---------------\n",
    "    # Convert soundfile to desired number of channels\n",
    "    #---------------\n",
    "    @staticmethod\n",
    "    def rechannel(aud, new_channel):\n",
    "        \n",
    "        sig, sr = aud\n",
    "        \n",
    "        if sig.shape[0] == new_channel:\n",
    "            #Nothing todo\n",
    "            return aud\n",
    "        \n",
    "        if (new_channel == 1):\n",
    "            #Convert stereo to mono by selecting only the first channel\n",
    "            resig = sig[:1, :]\n",
    "        else:\n",
    "            #Convert from mono to sterio by duplicating the first channel\n",
    "            resig = torch.cat([sig,sig])\n",
    "        return ((resig, sr))\n",
    "    \n",
    "    #---------------\n",
    "    #Resample to make sure samplerate is the same for all files - resample applies to one channel at a time\n",
    "    #---------------\n",
    "    @staticmethod\n",
    "    def resample(aud, newsr):\n",
    "        \n",
    "        sig, sr = aud\n",
    "        \n",
    "        if (sr == newsr):\n",
    "            #do nothing\n",
    "            return aud\n",
    "        \n",
    "        num_channels = sig.shape[0]\n",
    "        \n",
    "        #resample first channel\n",
    "        resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n",
    "        \n",
    "        if (num_channels > 1):\n",
    "            #Resample the second channel and merge both\n",
    "            retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n",
    "            resig = torch.cat([resig, retwo])\n",
    "            \n",
    "        return ((resig, newsr))\n",
    "    \n",
    "    \n",
    "    #-----------------\n",
    "    #Pad or turncate the signal to be off a standard length in milliseconds\n",
    "    #-----------------\n",
    "    @staticmethod\n",
    "    def pad_trunc(aud, max_ms):\n",
    "\n",
    "        sig, sr = aud\n",
    "        num_rows, sig_len = sig.shape\n",
    "        max_len = sr * max_ms / 1000\n",
    "        \n",
    "        if (sig_len > max_len):\n",
    "            #Turncate the signal to the given length\n",
    "            print(f\"Signal length larger than max length {sig_len} > {max_len}\")\n",
    "            sig = sig[:,:max_len]\n",
    "        elif (sig_len < max_len):\n",
    "            print(f\"Signal length smaller than max length {sig_len} < {max_len}\")\n",
    "\n",
    "            pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "            pad_end_len = max_len - sig_len - pad_begin_len\n",
    "            \n",
    "            #pad with zeroes\n",
    "            pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "            pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "            \n",
    "            sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "        \n",
    "        return (sig, sr)\n",
    "\n",
    "    #--------------------\n",
    "    #Shift the signal by a random bit, end of signal is wrapped around \n",
    "    #to beginning\n",
    "    #--------------------\n",
    "    @staticmethod\n",
    "    def time_shift(aud, shift_limit):\n",
    "        sig, sr = aud\n",
    "        \n",
    "        _, sig_len = sig.shape\n",
    "        shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "        return (sig.roll(shift_amt), sr)\n",
    "\n",
    "    #--------------------\n",
    "    #Add eccho or vibrato to the signal. \n",
    "    #This method is intented for the generation of synthetic data,\n",
    "    #using a limited dataset. \n",
    "    #--------------------\n",
    "    @staticmethod\n",
    "    def add_rand_effect(aud):\n",
    "        sig, sr = aud    \n",
    "        effect = random.randint(0,1)\n",
    "        if effect ==0:\n",
    "            delay = random.randint(40,100)\n",
    "            decay = round(random.uniform(0.2,0.7), 2)\n",
    "            effect = \"aecho=in_gain=0.8:out_gain=0.9:delays={}:decays={}\".format(delay, decay)\n",
    "        elif effect == 1:\n",
    "            freq = random.randint(9,15)\n",
    "            decay = round(random.uniform(0.15,0.4), 2)\n",
    "            effect = \"vibrato=f={}:d={}\".format(freq, decay)\n",
    "        effector = AudioEffector(effect=effect, pad_end=False)\n",
    "        sig_ef = effector.apply(sig.T, int(sr)) # int(sr)\n",
    "        sig_ef = sig_ef.T\n",
    "\n",
    "        return (sig_ef, sr)\n",
    "    \n",
    "    #----------------------------------\n",
    "    #Genetate spectrogram\n",
    "    #----------------------------------\n",
    "    @staticmethod\n",
    "    def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
    "        sig,sr = aud\n",
    "        top_db = 80\n",
    "        \n",
    "        #spec has shape [channel, n_mels, time]\n",
    "        spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "        \n",
    "        #convert to db\n",
    "        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "        \n",
    "        return spec\n",
    "    \n",
    "    \n",
    "    #Augment the spectrogram by masking out some sections of it in both the frequency\n",
    "    #dimencion (Horizontal) and the time dimension (vertical bars)\n",
    "    \n",
    "    @staticmethod\n",
    "    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "        _, n_mels, n_steps = spec.shape\n",
    "        mask_value = spec.mean()\n",
    "        aug_spec = spec\n",
    "        \n",
    "        freq_mask_param = max_mask_pct * n_mels\n",
    "        for _ in range(n_freq_masks):\n",
    "            aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "        \n",
    "        time_mask_param = max_mask_pct * n_steps\n",
    "        for _ in range(n_time_masks):\n",
    "            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "        \n",
    "        return np.absolute(aug_spec) # Jonas added np.absolute\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "id": "cea554cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechbrain.dataio.preprocess import AudioNormalizer\n",
    "\n",
    "class LoadTransform():\n",
    "    def __init__(self):\n",
    "        self.duration = 4000\n",
    "        self.sr_classifier = 16000 # this is the required sample rate for the classifier\n",
    "        self.channel = 1\n",
    "        self.shift_pct = 0.3\n",
    "        self.audio_normalizer = AudioNormalizer(sample_rate=self.sr_classifier)\n",
    "    \n",
    "    def load_audio(self, file_path, return_spectrogram=False):\n",
    "        signal = AudioUtil.open(file_path)\n",
    "        #print(f\"Audio tensor shape ={signal[0].shape}\")\n",
    "\n",
    "        signal = AudioUtil.rechannel(signal, self.channel)\n",
    "        #print(f\"rechan tensor shape ={signal[0].shape}\")\n",
    "\n",
    "        signal = AudioUtil.pad_trunc(signal, self.duration)\n",
    "        #print(f\"dur_aud tensor shape ={signal[0].shape}\")\n",
    "\n",
    "        signal = AudioUtil.time_shift(signal, self.shift_pct)\n",
    "        #print(f\"ts_aud tensor shape ={signal[0].shape}\")\n",
    "    \n",
    "        if return_spectrogram:\n",
    "            return AudioUtil.spectro_gram(signal)\n",
    "\n",
    "        signal, sr = signal[0].T, signal[1]\n",
    "        #print(f\"Audio sample rate ={sr}\")\n",
    "\n",
    "\n",
    "        return self.audio_normalizer(signal, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "id": "83280057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "#----------------\n",
    "#Sound dataset\n",
    "#----------------\n",
    "    \n",
    "class SoundDS(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        #self.df = df\n",
    "        #self.data_paths = data_paths\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.duration = 4000\n",
    "        self.sr = 44100\n",
    "        self.channel = 2\n",
    "        self.shift_pct = 0.3\n",
    "    \n",
    "    #------------------\n",
    "    #Number of items in dataset\n",
    "    #------------------\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    #------------------\n",
    "    #Get i'th item in dataset\n",
    "    #------------------\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the class ID  \n",
    "        #class_id = self.df.loc[idx, 'label']\n",
    "        #audio_file = str(self.df.loc[idx, 'filepath'])\n",
    "        audio_file = self.X[idx]\n",
    "        aud = AudioUtil.open(audio_file)\n",
    "        reaud = AudioUtil.resample(aud, self.sr)\n",
    "        rechan = AudioUtil.rechannel(reaud, self.channel)\n",
    "        dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n",
    "        ts_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n",
    "        y = torch.zeros(3)\n",
    "        y[cats.index(self.y[idx])] = 1.\n",
    "        #sgram = AudioUtil.spectro_gram(dur_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
    "        return ts_aud, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158356d3-c2b7-491f-a027-d693883c6e64",
   "metadata": {},
   "source": [
    "the classifier's `load_audio` function does some extra stuff like normalization, so let's use that \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "8fc677f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_huawei/air/air_8.wav'"
      ]
     },
     "execution_count": 696,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filep = df.iloc[0].filepath\n",
    "filep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "id": "d83dfb9c-27d1-45bd-a1d5-0d846ac88505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier tensor shape = torch.Size([64000])\n",
      "Transfrom tensor shape = torch.Size([64000])\n"
     ]
    }
   ],
   "source": [
    "tens = classifier.load_audio(sample.filepath)\n",
    "Transform = LoadTransform()\n",
    "tens2 = Transform.load_audio(sample.filepath)\n",
    "print(f\"Classifier tensor shape = {tens.shape}\")\n",
    "print(f\"Transfrom tensor shape = {tens2.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "id": "fbfdebe9-606e-4203-b98d-52fe04c3ddf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64000])"
      ]
     },
     "execution_count": 698,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tens2.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "id": "db9a63fd-465f-4bc1-9e92-61acfec23d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 192])"
      ]
     },
     "execution_count": 699,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_length =  torch.tensor([1.])\n",
    "enc = classifier.encode_batch(tens2.unsqueeze(0), rel_length)\n",
    "enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "id": "a41a0ea9-01e5-45f7-a135-a3723e718490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2608, -0.2772,  0.5087, -0.4518, -0.0299, -0.0104,  0.2589,  0.3234,\n",
       "          0.3436, -0.1196]])"
      ]
     },
     "execution_count": 700,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probabilities of each class\n",
    "preds = classifier.mods.classifier(enc).squeeze(1); preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "id": "de168ba5-2376-41c2-8f30-9a15341e47ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 701,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch of preds, but just 1 in the batch \n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "id": "90941511-53c3-4c9c-8ced-f72cb776d984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.3198, -0.2624,  0.4991, -0.4318, -0.0067, -0.0238,  0.2409,  0.3789,\n",
       "           0.3183, -0.1127]]),\n",
       " tensor([0.4991]),\n",
       " tensor([2]),\n",
       " ['air_conditioner'])"
      ]
     },
     "execution_count": 702,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that our work agrees with the full implementation \n",
    "classifier.classify_file(str(sample.filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace62738-b25a-4a0e-b999-843f2dd2090b",
   "metadata": {},
   "source": [
    "## Create dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "id": "a747206e-986d-4de7-9b4f-4e145e70cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "3bc48d3c-719b-4b5d-b851-7717aef3c47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['air', 'vac', 'off']"
      ]
     },
     "execution_count": 704,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "id": "c8ea7ebf-c071-4b6b-9bd3-d09f18e929ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>label</th>\n",
       "      <th>name</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>/Users/jonas/Library/CloudStorage/OneDrive-Uni...</td>\n",
       "      <td>off</td>\n",
       "      <td>off_8.wav</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>/Users/jonas/Library/CloudStorage/OneDrive-Uni...</td>\n",
       "      <td>off</td>\n",
       "      <td>off_12.wav</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>/Users/jonas/Library/CloudStorage/OneDrive-Uni...</td>\n",
       "      <td>air</td>\n",
       "      <td>air_1.wav</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             filepath label        name  \\\n",
       "81  /Users/jonas/Library/CloudStorage/OneDrive-Uni...   off   off_8.wav   \n",
       "84  /Users/jonas/Library/CloudStorage/OneDrive-Uni...   off  off_12.wav   \n",
       "29  /Users/jonas/Library/CloudStorage/OneDrive-Uni...   air   air_1.wav   \n",
       "\n",
       "    duration  \n",
       "81       4.0  \n",
       "84       4.0  \n",
       "29       4.0  "
      ]
     },
     "execution_count": 705,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "id": "8d2e20f5-d386-4370-9a8b-16d1ba8cbcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import cache\n",
    "\n",
    "class ApplianceDS(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y \n",
    "    def __len__(self): return len(self.y)\n",
    "    \n",
    "    @cache\n",
    "    def __getitem__(self, i):\n",
    "        y = torch.zeros(len(cats))\n",
    "        y[cats.index(self.y[i])] = 1.\n",
    "        return self.X[i], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "id": "c1ed94bc-abf6-4048-81f5-cd521f512be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([69, 64000]), torch.Size([18, 64000]))"
      ]
     },
     "execution_count": 707,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Xtrain = torch.stack([classifier.load_audio(fp) for fp in train.filepath])\n",
    "#Xtrain = torch.stack([classifier.load_audio(fp) for fp in train.filepath])\n",
    "Xtrain = torch.stack([Transform.load_audio(fp) for fp in train.filepath])\n",
    "Xval = torch.stack([Transform.load_audio(fp) for fp in val.filepath])\n",
    "#Xtest = torch.stack([Transform.load_audio(fp) for fp in test.filepath])\n",
    "\n",
    "ytrain = list(train.label)\n",
    "yval = list(val.label)\n",
    "#ytest = list(test.label)\n",
    "Xtrain.shape, Xval.shape\n",
    "\n",
    "\n",
    "# (torch.Size([59, 64000]), torch.Size([13, 64000]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "id": "066e183d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['air',\n",
       " 'off',\n",
       " 'air',\n",
       " 'air',\n",
       " 'off',\n",
       " 'off',\n",
       " 'off',\n",
       " 'off',\n",
       " 'vac',\n",
       " 'air',\n",
       " 'vac',\n",
       " 'air',\n",
       " 'air',\n",
       " 'vac',\n",
       " 'vac',\n",
       " 'vac',\n",
       " 'vac',\n",
       " 'off']"
      ]
     },
     "execution_count": 708,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "id": "ec66f0ba-8b7b-49ad-8d45-080ff254c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ApplianceDS(Xtrain, ytrain)\n",
    "val_ds = ApplianceDS(Xval, yval)\n",
    "#test_ds = ApplianceDS(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "id": "7bed9fe4-1855-4cd6-9932-ffb474d144e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "# we don't need batches for the validation set so just put into a big batch \n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=len(val_ds), shuffle=True)\n",
    "#test_dl = torch.utils.data.DataLoader(test_ds, batch_size=len(test_ds), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "id": "a56a718c-6710-4e09-be35-66cc8c12c7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([18, 64000]), torch.Size([18, 3]))"
      ]
     },
     "execution_count": 734,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xval, yval = next(iter(val_dl))\n",
    "Xval.shape, yval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "id": "735317d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights():\n",
    "    classifier.mods.classifier.weight = torch.nn.Parameter(torch.FloatTensor(len(cats), 192))\n",
    "    torch.nn.init.xavier_uniform_(classifier.mods.classifier.weight)\n",
    "reset_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "id": "a310d9f7-d007-41f1-a717-18abab7dec56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "for param in classifier.mods.classifier.parameters():\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "id": "0a2cf1ee-167b-4824-87a6-4cd4e1bec357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fbank(\n",
       "  (compute_STFT): STFT()\n",
       "  (compute_fbanks): Filterbank()\n",
       "  (compute_deltas): Deltas()\n",
       "  (context_window): ContextWindow()\n",
       ")"
      ]
     },
     "execution_count": 737,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.mods.compute_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "id": "6dac0339-f586-4f66-925d-540af1ed5b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute_features\n",
      "mean_var_norm\n",
      "embedding_model\n",
      "embedding_model False\n",
      "classifier\n",
      "classifier True\n"
     ]
    }
   ],
   "source": [
    "for mod in classifier.mods:\n",
    "    print(mod)\n",
    "    for p in classifier.mods.__getattr__(mod).parameters():\n",
    "        print(mod, p.requires_grad)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "id": "6a96b8de-0ed2-4088-8f92-669771ea7d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "id": "7e6f500f-6d2a-41da-8e76-bca22eb9a408",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:19<31:23, 19.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 1.0925, Validation Loss: 1.0824, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:37<30:31, 18.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.0817, Validation Loss: 1.0736, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:55<29:56, 18.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 1.0737, Validation Loss: 1.0646, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [01:14<29:32, 18.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 1.0643, Validation Loss: 1.0561, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [01:33<29:33, 18.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 1.0550, Validation Loss: 1.0483, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [01:51<29:09, 18.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 1.0476, Validation Loss: 1.0413, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [02:10<28:56, 18.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 1.0401, Validation Loss: 1.0349, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [02:28<28:31, 18.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 1.0338, Validation Loss: 1.0288, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [02:47<28:20, 18.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 1.0282, Validation Loss: 1.0233, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [03:06<28:11, 18.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 1.0218, Validation Loss: 1.0182, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [03:27<28:48, 19.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 1.0173, Validation Loss: 1.0139, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [03:46<28:19, 19.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss: 1.0147, Validation Loss: 1.0099, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [04:07<28:41, 19.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss: 1.0096, Validation Loss: 1.0059, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [04:28<28:41, 20.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss: 1.0077, Validation Loss: 1.0029, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [04:47<27:57, 19.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss: 1.0011, Validation Loss: 0.9997, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [05:06<27:27, 19.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss: 0.9984, Validation Loss: 0.9970, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [05:26<27:08, 19.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss: 0.9976, Validation Loss: 0.9945, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [05:44<26:24, 19.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss: 0.9952, Validation Loss: 0.9922, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [06:03<25:51, 19.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss: 0.9925, Validation Loss: 0.9901, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [06:22<25:15, 18.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss: 0.9870, Validation Loss: 0.9884, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [06:41<25:06, 19.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss: 0.9876, Validation Loss: 0.9866, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [07:00<24:45, 19.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Loss: 0.9864, Validation Loss: 0.9851, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [07:20<24:57, 19.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Train Loss: 0.9841, Validation Loss: 0.9838, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [07:40<24:31, 19.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Train Loss: 0.9845, Validation Loss: 0.9824, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [07:58<23:58, 19.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Train Loss: 0.9812, Validation Loss: 0.9813, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [08:17<23:29, 19.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Train Loss: 0.9808, Validation Loss: 0.9804, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [08:37<23:20, 19.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Train Loss: 0.9796, Validation Loss: 0.9795, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [08:55<22:57, 19.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Train Loss: 0.9811, Validation Loss: 0.9787, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [09:15<22:41, 19.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Train Loss: 0.9780, Validation Loss: 0.9776, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [09:34<22:25, 19.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train Loss: 0.9788, Validation Loss: 0.9768, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [09:52<21:48, 18.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Train Loss: 0.9756, Validation Loss: 0.9762, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [10:11<21:16, 18.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: Train Loss: 0.9763, Validation Loss: 0.9759, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [10:30<20:59, 18.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: Train Loss: 0.9751, Validation Loss: 0.9754, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [10:49<20:50, 18.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: Train Loss: 0.9755, Validation Loss: 0.9745, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [11:08<20:32, 18.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: Train Loss: 0.9755, Validation Loss: 0.9741, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [11:27<20:15, 18.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: Train Loss: 0.9730, Validation Loss: 0.9736, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [11:46<19:55, 18.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: Train Loss: 0.9743, Validation Loss: 0.9732, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38/100 [12:04<19:27, 18.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: Train Loss: 0.9747, Validation Loss: 0.9729, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [12:23<19:00, 18.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: Train Loss: 0.9717, Validation Loss: 0.9725, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [12:41<18:36, 18.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: Train Loss: 0.9696, Validation Loss: 0.9721, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [13:00<18:15, 18.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: Train Loss: 0.9724, Validation Loss: 0.9717, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [13:18<17:55, 18.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: Train Loss: 0.9711, Validation Loss: 0.9715, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [13:37<17:39, 18.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: Train Loss: 0.9707, Validation Loss: 0.9712, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [13:55<17:18, 18.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: Train Loss: 0.9713, Validation Loss: 0.9708, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [14:14<17:00, 18.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: Train Loss: 0.9702, Validation Loss: 0.9704, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [14:32<16:39, 18.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: Train Loss: 0.9702, Validation Loss: 0.9702, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 47/100 [14:51<16:19, 18.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: Train Loss: 0.9696, Validation Loss: 0.9700, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48/100 [15:09<16:00, 18.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: Train Loss: 0.9690, Validation Loss: 0.9702, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49/100 [15:28<15:43, 18.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: Train Loss: 0.9697, Validation Loss: 0.9699, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [15:46<15:24, 18.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: Train Loss: 0.9716, Validation Loss: 0.9694, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [16:05<15:05, 18.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Train Loss: 0.9688, Validation Loss: 0.9693, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [16:23<14:45, 18.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: Train Loss: 0.9690, Validation Loss: 0.9691, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53/100 [16:42<14:28, 18.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52: Train Loss: 0.9693, Validation Loss: 0.9690, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54/100 [17:01<14:17, 18.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53: Train Loss: 0.9700, Validation Loss: 0.9690, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [17:20<14:04, 18.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: Train Loss: 0.9686, Validation Loss: 0.9687, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 56/100 [17:39<13:46, 18.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55: Train Loss: 0.9682, Validation Loss: 0.9687, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 57/100 [17:57<13:26, 18.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56: Train Loss: 0.9688, Validation Loss: 0.9687, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58/100 [18:16<13:06, 18.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57: Train Loss: 0.9674, Validation Loss: 0.9686, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 59/100 [18:34<12:46, 18.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58: Train Loss: 0.9691, Validation Loss: 0.9684, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [18:53<12:25, 18.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: Train Loss: 0.9682, Validation Loss: 0.9683, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [19:12<12:06, 18.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60: Train Loss: 0.9677, Validation Loss: 0.9683, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [19:30<11:51, 18.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61: Train Loss: 0.9673, Validation Loss: 0.9683, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 63/100 [19:49<11:33, 18.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62: Train Loss: 0.9686, Validation Loss: 0.9680, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 64/100 [20:08<11:13, 18.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63: Train Loss: 0.9681, Validation Loss: 0.9680, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65/100 [20:27<10:53, 18.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64: Train Loss: 0.9669, Validation Loss: 0.9681, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 66/100 [20:45<10:35, 18.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65: Train Loss: 0.9672, Validation Loss: 0.9678, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 67/100 [21:04<10:16, 18.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66: Train Loss: 0.9666, Validation Loss: 0.9676, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 68/100 [21:23<09:58, 18.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67: Train Loss: 0.9657, Validation Loss: 0.9676, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 69/100 [21:41<09:40, 18.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68: Train Loss: 0.9670, Validation Loss: 0.9677, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [38:10<2:34:49, 309.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69: Train Loss: 0.9677, Validation Loss: 0.9676, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [38:29<1:47:31, 222.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70: Train Loss: 0.9684, Validation Loss: 0.9676, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72/100 [48:13<2:34:24, 330.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71: Train Loss: 0.9666, Validation Loss: 0.9675, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73/100 [48:32<1:46:45, 237.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72: Train Loss: 0.9679, Validation Loss: 0.9673, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 74/100 [48:51<1:14:25, 171.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73: Train Loss: 0.9667, Validation Loss: 0.9674, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75/100 [49:09<52:25, 125.84s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: Train Loss: 0.9694, Validation Loss: 0.9676, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 76/100 [49:28<37:28, 93.69s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75: Train Loss: 0.9671, Validation Loss: 0.9673, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 77/100 [49:47<27:21, 71.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76: Train Loss: 0.9663, Validation Loss: 0.9675, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 78/100 [50:07<20:29, 55.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77: Train Loss: 0.9669, Validation Loss: 0.9672, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 79/100 [50:27<15:48, 45.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78: Train Loss: 0.9651, Validation Loss: 0.9671, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [50:48<12:37, 37.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79: Train Loss: 0.9679, Validation Loss: 0.9670, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [51:09<10:21, 32.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80: Train Loss: 0.9659, Validation Loss: 0.9670, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82/100 [51:30<08:46, 29.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81: Train Loss: 0.9669, Validation Loss: 0.9671, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83/100 [51:52<07:40, 27.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82: Train Loss: 0.9657, Validation Loss: 0.9672, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 84/100 [52:14<06:51, 25.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83: Train Loss: 0.9656, Validation Loss: 0.9672, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85/100 [52:36<06:09, 24.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84: Train Loss: 0.9674, Validation Loss: 0.9671, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [1:09:15<1:13:56, 316.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85: Train Loss: 0.9658, Validation Loss: 0.9671, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 87/100 [1:09:34<49:17, 227.47s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86: Train Loss: 0.9651, Validation Loss: 0.9671, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 88/100 [1:09:53<32:57, 164.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87: Train Loss: 0.9655, Validation Loss: 0.9672, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 89/100 [1:10:11<22:09, 120.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88: Train Loss: 0.9673, Validation Loss: 0.9672, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [1:14:56<28:20, 170.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89: Train Loss: 0.9660, Validation Loss: 0.9672, Learning rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [1:15:15<18:43, 124.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90: Train Loss: 0.9654, Validation Loss: 0.9671, Learning rate: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [1:15:30<07:28, 49.79s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[744], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     15\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(losses))\n\u001b[0;32m---> 17\u001b[0m val_preds, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXval\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m     18\u001b[0m val_loss_ \u001b[38;5;241m=\u001b[39m loss_fn(val_preds, yval)\n\u001b[1;32m     19\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep(val_loss_)\n",
      "File \u001b[0;32m~/.virtualenvs/labear/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/labear/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/labear/lib/python3.11/site-packages/speechbrain/inference/classifiers.py:184\u001b[0m, in \u001b[0;36mEncoderClassifier.forward\u001b[0;34m(self, wavs, wav_lens)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, wavs, wav_lens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    183\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs the classification\"\"\"\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassify_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwavs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwav_lens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/labear/lib/python3.11/site-packages/speechbrain/inference/classifiers.py:146\u001b[0m, in \u001b[0;36mEncoderClassifier.classify_batch\u001b[0;34m(self, wavs, wav_lens)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclassify_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, wavs, wav_lens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    118\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Performs classification on the top of the encoded features.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m    It returns the posterior probabilities, the index and, if the label\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m        (label encoder should be provided).\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m     emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwavs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwav_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     out_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmods\u001b[38;5;241m.\u001b[39mclassifier(emb)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    148\u001b[0m     score, index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(out_prob, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.virtualenvs/labear/lib/python3.11/site-packages/speechbrain/inference/classifiers.py:110\u001b[0m, in \u001b[0;36mEncoderClassifier.encode_batch\u001b[0;34m(self, wavs, wav_lens, normalize)\u001b[0m\n\u001b[1;32m    108\u001b[0m feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmods\u001b[38;5;241m.\u001b[39mcompute_features(wavs)\n\u001b[1;32m    109\u001b[0m feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmods\u001b[38;5;241m.\u001b[39mmean_var_norm(feats, wav_lens)\n\u001b[0;32m--> 110\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwav_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalize:\n\u001b[1;32m    112\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams\u001b[38;5;241m.\u001b[39mmean_var_norm_emb(\n\u001b[1;32m    113\u001b[0m         embeddings, torch\u001b[38;5;241m.\u001b[39mones(embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    114\u001b[0m     )\n",
      "File \u001b[0;32m~/.virtualenvs/labear/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/labear/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/labear/lib/python3.11/site-packages/speechbrain/lobes/models/ECAPA_TDNN.py:493\u001b[0m, in \u001b[0;36mECAPA_TDNN.forward\u001b[0;34m(self, x, lengths)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;66;03m# Multi-layer feature aggregation\u001b[39;00m\n\u001b[1;32m    492\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(xl[\u001b[38;5;241m1\u001b[39m:], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 493\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmfa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;66;03m# Attentive Statistical Pooling\u001b[39;00m\n\u001b[1;32m    496\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masp(x, lengths\u001b[38;5;241m=\u001b[39mlengths)\n",
      "File \u001b[0;32m~/.virtualenvs/labear/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/labear/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/labear/lib/python3.11/site-packages/speechbrain/lobes/models/ECAPA_TDNN.py:80\u001b[0m, in \u001b[0;36mTDNNBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Processes the input tensor x and returns an output tensor.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[0;32m~/.virtualenvs/labear/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/labear/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/labear/lib/python3.11/site-packages/speechbrain/nnet/CNN.py:445\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    441\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPadding must be \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcausal\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding\n\u001b[1;32m    443\u001b[0m     )\n\u001b[0;32m--> 445\u001b[0m wx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munsqueeze:\n\u001b[1;32m    448\u001b[0m     wx \u001b[38;5;241m=\u001b[39m wx\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.virtualenvs/labear/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/labear/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/labear/lib/python3.11/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/labear/lib/python3.11/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "reset_weights()\n",
    "optimizer = torch.optim.Adam(classifier.mods.classifier.parameters(), lr=0.002) # changed from 0.001\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)\n",
    "for epoch in tqdm(range(100)):\n",
    "    losses = []\n",
    "    for batch_idx, (X, y) in enumerate(train_dl):\n",
    "        logits, confidences, classes, decoded_classes = classifier(X)\n",
    "        loss = loss_fn(logits, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    train_loss.append(np.mean(losses))\n",
    "    \n",
    "    val_preds, *_ = classifier(Xval)    \n",
    "    val_loss_ = loss_fn(val_preds, yval)\n",
    "    scheduler.step(val_loss_)\n",
    "    val_loss.append(val_loss_.item())\n",
    "\n",
    "    print(f\"Epoch {epoch}: Train Loss: {train_loss[-1]:.4f}, Validation Loss: {val_loss[-1]:.4f}, Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "id": "7fa6a70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:18<31:16, 18.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 1.0987, Validation Loss: 1.0925\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:37<30:42, 18.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.0477, Validation Loss: 1.0768\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:56<30:14, 18.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 1.0068, Validation Loss: 1.0431\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [01:14<29:45, 18.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.9676, Validation Loss: 0.9917\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [01:33<29:27, 18.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.9395, Validation Loss: 0.9353\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [01:52<29:32, 18.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.9035, Validation Loss: 0.8951\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [02:12<29:30, 19.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.8695, Validation Loss: 0.8669\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [02:32<29:53, 19.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.8528, Validation Loss: 0.8423\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [02:51<29:30, 19.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.8401, Validation Loss: 0.8215\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [03:10<28:56, 19.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 0.8218, Validation Loss: 0.8050\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [03:30<28:40, 19.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 0.7969, Validation Loss: 0.7911\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [03:49<28:15, 19.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss: 0.7706, Validation Loss: 0.7799\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [04:08<28:01, 19.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss: 0.7523, Validation Loss: 0.7685\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [04:28<27:43, 19.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss: 0.7492, Validation Loss: 0.7583\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [04:47<27:13, 19.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss: 0.7580, Validation Loss: 0.7491\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [05:06<26:48, 19.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss: 0.7359, Validation Loss: 0.7431\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [05:25<26:39, 19.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss: 0.7440, Validation Loss: 0.7382\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [05:44<26:11, 19.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss: 0.7105, Validation Loss: 0.7330\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [06:03<25:42, 19.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss: 0.7234, Validation Loss: 0.7293\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [06:25<26:34, 19.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss: 0.7245, Validation Loss: 0.7259\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [06:46<26:36, 20.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss: 0.7325, Validation Loss: 0.7224\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [07:06<26:09, 20.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Loss: 0.7270, Validation Loss: 0.7206\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [07:26<26:03, 20.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Train Loss: 0.7141, Validation Loss: 0.7177\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [07:46<25:21, 20.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Train Loss: 0.6992, Validation Loss: 0.7161\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [08:06<25:11, 20.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Train Loss: 0.7017, Validation Loss: 0.7154\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [08:27<25:09, 20.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Train Loss: 0.7377, Validation Loss: 0.7119\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [08:47<24:34, 20.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Train Loss: 0.7530, Validation Loss: 0.7106\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [09:07<24:10, 20.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Train Loss: 0.7200, Validation Loss: 0.7106\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [09:26<23:36, 19.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Train Loss: 0.6913, Validation Loss: 0.7114\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [09:47<23:27, 20.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train Loss: 0.7311, Validation Loss: 0.7098\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [10:06<22:54, 19.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Train Loss: 0.7047, Validation Loss: 0.7111\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [10:26<22:22, 19.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: Train Loss: 0.7440, Validation Loss: 0.7103\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [10:45<21:51, 19.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: Train Loss: 0.7090, Validation Loss: 0.7106\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [11:04<21:20, 19.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: Train Loss: 0.7052, Validation Loss: 0.7105\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [11:23<20:59, 19.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: Train Loss: 0.7048, Validation Loss: 0.7112\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [11:44<21:10, 19.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: Train Loss: 0.7500, Validation Loss: 0.7093\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [12:04<20:55, 19.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: Train Loss: 0.7458, Validation Loss: 0.7083\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38/100 [12:24<20:32, 19.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: Train Loss: 0.6866, Validation Loss: 0.7105\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [12:44<20:16, 19.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: Train Loss: 0.7056, Validation Loss: 0.7116\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [13:05<20:10, 20.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: Train Loss: 0.7014, Validation Loss: 0.7111\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [13:25<19:44, 20.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: Train Loss: 0.7101, Validation Loss: 0.7105\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [13:44<19:04, 19.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: Train Loss: 0.6961, Validation Loss: 0.7120\n",
      "Learning Rate: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [14:03<18:43, 19.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: Train Loss: 0.7282, Validation Loss: 0.7115\n",
      "Learning Rate: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [14:22<18:14, 19.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: Train Loss: 0.7239, Validation Loss: 0.7107\n",
      "Learning Rate: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [14:42<17:59, 19.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: Train Loss: 0.6856, Validation Loss: 0.7107\n",
      "Learning Rate: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [15:01<17:32, 19.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: Train Loss: 0.7203, Validation Loss: 0.7111\n",
      "Learning Rate: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 47/100 [15:20<17:03, 19.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: Train Loss: 0.7055, Validation Loss: 0.7107\n",
      "Learning Rate: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48/100 [15:40<16:56, 19.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: Train Loss: 0.7073, Validation Loss: 0.7113\n",
      "Learning Rate: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49/100 [16:00<16:41, 19.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: Train Loss: 0.7066, Validation Loss: 0.7103\n",
      "Learning Rate: 2e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [16:20<16:21, 19.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: Train Loss: 0.7086, Validation Loss: 0.7089\n",
      "Learning Rate: 2e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [16:40<16:10, 19.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Train Loss: 0.6790, Validation Loss: 0.7107\n",
      "Learning Rate: 2e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [16:59<15:33, 19.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: Train Loss: 0.7255, Validation Loss: 0.7083\n",
      "Learning Rate: 2e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53/100 [17:19<15:20, 19.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52: Train Loss: 0.6832, Validation Loss: 0.7110\n",
      "Learning Rate: 2e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54/100 [17:38<14:57, 19.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53: Train Loss: 0.7228, Validation Loss: 0.7118\n",
      "Learning Rate: 2e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [17:57<14:35, 19.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: Train Loss: 0.7205, Validation Loss: 0.7090\n",
      "Learning Rate: 2.0000000000000003e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 56/100 [18:18<14:36, 19.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55: Train Loss: 0.7366, Validation Loss: 0.7083\n",
      "Learning Rate: 2.0000000000000003e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 57/100 [18:38<14:09, 19.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56: Train Loss: 0.6888, Validation Loss: 0.7091\n",
      "Learning Rate: 2.0000000000000003e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58/100 [18:57<13:47, 19.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57: Train Loss: 0.7021, Validation Loss: 0.7104\n",
      "Learning Rate: 2.0000000000000003e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 59/100 [19:18<13:39, 19.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58: Train Loss: 0.6883, Validation Loss: 0.7127\n",
      "Learning Rate: 2.0000000000000003e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [19:37<13:05, 19.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: Train Loss: 0.6967, Validation Loss: 0.7125\n",
      "Learning Rate: 2.0000000000000003e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [19:56<12:44, 19.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60: Train Loss: 0.7122, Validation Loss: 0.7106\n",
      "Learning Rate: 2.0000000000000004e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [20:16<12:31, 19.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61: Train Loss: 0.7032, Validation Loss: 0.7109\n",
      "Learning Rate: 2.0000000000000004e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 63/100 [20:35<12:03, 19.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62: Train Loss: 0.7134, Validation Loss: 0.7101\n",
      "Learning Rate: 2.0000000000000004e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 64/100 [20:55<11:43, 19.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63: Train Loss: 0.7226, Validation Loss: 0.7091\n",
      "Learning Rate: 2.0000000000000004e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65/100 [21:14<11:17, 19.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64: Train Loss: 0.7386, Validation Loss: 0.7096\n",
      "Learning Rate: 2.0000000000000004e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 66/100 [21:33<10:55, 19.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65: Train Loss: 0.7162, Validation Loss: 0.7095\n",
      "Learning Rate: 2.0000000000000004e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 67/100 [21:52<10:31, 19.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66: Train Loss: 0.7150, Validation Loss: 0.7096\n",
      "Learning Rate: 2.0000000000000007e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 68/100 [22:11<10:11, 19.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67: Train Loss: 0.7149, Validation Loss: 0.7111\n",
      "Learning Rate: 2.0000000000000007e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 69/100 [22:29<09:48, 18.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68: Train Loss: 0.7172, Validation Loss: 0.7109\n",
      "Learning Rate: 2.0000000000000007e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [22:49<09:33, 19.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69: Train Loss: 0.7064, Validation Loss: 0.7099\n",
      "Learning Rate: 2.0000000000000007e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [23:09<09:20, 19.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70: Train Loss: 0.7127, Validation Loss: 0.7088\n",
      "Learning Rate: 2.0000000000000007e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72/100 [23:28<09:00, 19.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71: Train Loss: 0.7412, Validation Loss: 0.7074\n",
      "Learning Rate: 2.0000000000000007e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73/100 [23:47<08:40, 19.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72: Train Loss: 0.7425, Validation Loss: 0.7094\n",
      "Learning Rate: 2.0000000000000007e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 74/100 [24:06<08:17, 19.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73: Train Loss: 0.7268, Validation Loss: 0.7085\n",
      "Learning Rate: 2.0000000000000007e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75/100 [24:25<07:57, 19.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: Train Loss: 0.7377, Validation Loss: 0.7076\n",
      "Learning Rate: 2.0000000000000007e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 76/100 [24:44<07:35, 18.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75: Train Loss: 0.7553, Validation Loss: 0.7060\n",
      "Learning Rate: 2.0000000000000007e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 77/100 [25:03<07:15, 18.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76: Train Loss: 0.7292, Validation Loss: 0.7080\n",
      "Learning Rate: 2.0000000000000007e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 78/100 [25:21<06:56, 18.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77: Train Loss: 0.7304, Validation Loss: 0.7106\n",
      "Learning Rate: 2.0000000000000007e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 79/100 [25:41<06:38, 19.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78: Train Loss: 0.7574, Validation Loss: 0.7093\n",
      "Learning Rate: 2.0000000000000007e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [26:01<06:28, 19.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79: Train Loss: 0.7297, Validation Loss: 0.7100\n",
      "Learning Rate: 2.0000000000000007e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [26:22<06:16, 19.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80: Train Loss: 0.7190, Validation Loss: 0.7108\n",
      "Learning Rate: 2.0000000000000007e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82/100 [26:41<05:54, 19.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81: Train Loss: 0.6880, Validation Loss: 0.7106\n",
      "Learning Rate: 2.000000000000001e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83/100 [27:02<05:39, 19.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82: Train Loss: 0.7024, Validation Loss: 0.7102\n",
      "Learning Rate: 2.000000000000001e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 84/100 [27:23<05:24, 20.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83: Train Loss: 0.7492, Validation Loss: 0.7082\n",
      "Learning Rate: 2.000000000000001e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85/100 [27:44<05:07, 20.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84: Train Loss: 0.7171, Validation Loss: 0.7101\n",
      "Learning Rate: 2.000000000000001e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [28:03<04:43, 20.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85: Train Loss: 0.7154, Validation Loss: 0.7101\n",
      "Learning Rate: 2.000000000000001e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 87/100 [28:22<04:18, 19.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86: Train Loss: 0.7329, Validation Loss: 0.7101\n",
      "Learning Rate: 2.000000000000001e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 88/100 [28:41<03:54, 19.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87: Train Loss: 0.7495, Validation Loss: 0.7094\n",
      "Learning Rate: 2.000000000000001e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 89/100 [29:00<03:33, 19.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88: Train Loss: 0.7172, Validation Loss: 0.7109\n",
      "Learning Rate: 2.000000000000001e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [29:20<03:13, 19.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89: Train Loss: 0.6977, Validation Loss: 0.7106\n",
      "Learning Rate: 2.000000000000001e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [29:38<02:52, 19.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90: Train Loss: 0.7127, Validation Loss: 0.7091\n",
      "Learning Rate: 2.000000000000001e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92/100 [29:57<02:32, 19.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91: Train Loss: 0.6947, Validation Loss: 0.7092\n",
      "Learning Rate: 2.000000000000001e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 93/100 [30:16<02:13, 19.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92: Train Loss: 0.7165, Validation Loss: 0.7094\n",
      "Learning Rate: 2.000000000000001e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 94/100 [30:35<01:54, 19.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93: Train Loss: 0.6938, Validation Loss: 0.7114\n",
      "Learning Rate: 2.000000000000001e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95/100 [30:54<01:34, 19.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94: Train Loss: 0.6951, Validation Loss: 0.7103\n",
      "Learning Rate: 2.000000000000001e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 96/100 [31:13<01:16, 19.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95: Train Loss: 0.7392, Validation Loss: 0.7094\n",
      "Learning Rate: 2.000000000000001e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 97/100 [31:32<00:57, 19.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96: Train Loss: 0.6925, Validation Loss: 0.7104\n",
      "Learning Rate: 2.000000000000001e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 98/100 [31:51<00:37, 19.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97: Train Loss: 0.7085, Validation Loss: 0.7107\n",
      "Learning Rate: 2.000000000000001e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 99/100 [32:10<00:18, 18.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98: Train Loss: 0.7105, Validation Loss: 0.7096\n",
      "Learning Rate: 2.000000000000001e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [32:29<00:00, 19.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: Train Loss: 0.7437, Validation Loss: 0.7078\n",
      "Learning Rate: 2.000000000000001e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "# Reset weights before training\n",
    "reset_weights()\n",
    "\n",
    "# Optimizer with modified learning rate\n",
    "optimizer = torch.optim.Adam(classifier.mods.classifier.parameters(), lr=0.002)  # Changed from 0.001\n",
    "\n",
    "# Scheduler to reduce learning rate on plateau\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(100)):\n",
    "    # Set model to training mode\n",
    "    classifier.train()\n",
    "    losses = []\n",
    "    \n",
    "    for batch_idx, (X, y) in enumerate(train_dl):\n",
    "        logits, confidences, classes, decoded_classes = classifier(X)\n",
    "        loss = loss_fn(logits, y)\n",
    "        \n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        loss.backward()        # Backpropagate\n",
    "        optimizer.step()       # Update parameters\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    train_loss.append(np.mean(losses))\n",
    "\n",
    "    # Validation step\n",
    "    classifier.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculation for validation\n",
    "        val_preds, *_ = classifier(Xval)\n",
    "        val_loss_ = loss_fn(val_preds, yval)\n",
    "        val_loss.append(val_loss_.item())\n",
    "    \n",
    "    # Print loss\n",
    "    print(f\"Epoch {epoch}: Train Loss: {train_loss[-1]:.4f}, Validation Loss: {val_loss[-1]:.4f}\")\n",
    "\n",
    "    # Step the scheduler with the validation loss\n",
    "    scheduler.step(val_loss_)\n",
    "\n",
    "    # Optional: print the current learning rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(f\"Learning Rate: {param_group['lr']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "id": "1ed48124-7fde-4c6c-93a4-41cd1fb56b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.plot(val_loss)\n",
    "plt.legend(['train','val'])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35711b3-401c-4668-bbee-610fb0385998",
   "metadata": {},
   "source": [
    "### Validation accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "id": "47c12d93-65b3-4594-9903-7f2a5b1a7f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['air', 'vac', 'off']"
      ]
     },
     "execution_count": 748,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "id": "d0c5d6d1-998e-4c70-8e12-de4d5e60fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, *rest = classifier(Xval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "id": "f4ffa15a-3cad-437b-bae0-a4cfd51b1470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 1, 2, 1, 1, 2, 2, 1, 0, 0, 1, 0, 2, 0, 2, 2, 0])"
      ]
     },
     "execution_count": 750,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "id": "0df37bba-6218-498b-8e7e-4ea1e932cb42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 1, 2, 1, 1, 2, 0, 1, 0, 0, 1, 0, 2, 0, 2, 2, 0])"
      ]
     },
     "execution_count": 751,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yval.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "id": "991325d5-fc70-4d4f-84a6-94b4ec8a1ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9444)"
      ]
     },
     "execution_count": 752,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds.argmax(axis=1) == yval.argmax(axis=1)).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cff356d-9e15-4e34-b764-c7e862ffb552",
   "metadata": {},
   "source": [
    "### Train accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "id": "872d2a4c-113d-4302-bba0-e676120d1e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 0, 1, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 1, 2, 0, 0, 1, 2, 1, 0, 1, 2,\n",
       "        1, 0, 1, 2, 2, 2, 0, 2, 1, 0, 2, 0, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 2, 0,\n",
       "        1, 2, 0, 1, 1, 0, 1, 0, 1, 2, 1, 2, 0, 0, 0, 2, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 753,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([cats.index(val) for val in ytrain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "id": "6538e28f-09bd-48ed-92d8-3c8714aac3f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 0, 1, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 1, 2, 0, 0, 1, 2, 1, 0, 1, 2,\n",
       "        1, 0, 1, 2, 2, 2, 0, 2, 1, 0, 2, 0, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 2, 0,\n",
       "        1, 2, 0, 1, 1, 0, 1, 0, 1, 2, 1, 2, 0, 0, 0, 2, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 754,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preds, *rest = classifier(Xtrain)\n",
    "train_preds.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "id": "98f3075c-3c42-4103-86a1-509309309445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 755,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.tensor([cats.index(val) for val in ytrain]) == train_preds.argmax(axis=1)).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8662e8cf",
   "metadata": {},
   "source": [
    "# Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "id": "741eb91d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xcvxcvytest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[756], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mxcvxcvytest\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xcvxcvytest' is not defined"
     ]
    }
   ],
   "source": [
    "xcvxcvytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca5ad0c-7686-4c2d-9bdd-ebe63bc1b2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4167)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([cats.index(val) for val in ytest])\n",
    "test_preds, *rest = classifier(Xtest)\n",
    "test_preds.argmax(axis=1)\n",
    "(torch.tensor([cats.index(val) for val in ytest]) == test_preds.argmax(axis=1)).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1776fff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new test data\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "path = Path(\"/Users/jonas/Library/CloudStorage/OneDrive-UniversityofExeter/sound_recognition/g28_test\")\n",
    "files = list(path.rglob(\"*.wav\"))\n",
    "df_test = pd.DataFrame({\"filepath\":files})\n",
    "df_test[\"label\"] = df_test[\"filepath\"].apply(lambda x: Path(x).parent.name)\n",
    "df_test[\"duration\"] = df_test[\"filepath\"].apply(lambda x: librosa.get_duration(path=x))\n",
    "df_test[\"name\"] = df_test[\"filepath\"].apply(lambda x: Path(x).name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415c5a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>label</th>\n",
       "      <th>duration</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>/Users/jonas/Library/CloudStorage/OneDrive-Uni...</td>\n",
       "      <td>off</td>\n",
       "      <td>4.0</td>\n",
       "      <td>off_3.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/jonas/Library/CloudStorage/OneDrive-Uni...</td>\n",
       "      <td>air</td>\n",
       "      <td>4.0</td>\n",
       "      <td>air_3.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/Users/jonas/Library/CloudStorage/OneDrive-Uni...</td>\n",
       "      <td>vac</td>\n",
       "      <td>4.0</td>\n",
       "      <td>vac_1.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/Users/jonas/Library/CloudStorage/OneDrive-Uni...</td>\n",
       "      <td>vac</td>\n",
       "      <td>4.0</td>\n",
       "      <td>vac_0.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/Users/jonas/Library/CloudStorage/OneDrive-Uni...</td>\n",
       "      <td>vac</td>\n",
       "      <td>4.0</td>\n",
       "      <td>vac_3.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             filepath label  duration  \\\n",
       "17  /Users/jonas/Library/CloudStorage/OneDrive-Uni...   off       4.0   \n",
       "3   /Users/jonas/Library/CloudStorage/OneDrive-Uni...   air       4.0   \n",
       "8   /Users/jonas/Library/CloudStorage/OneDrive-Uni...   vac       4.0   \n",
       "9   /Users/jonas/Library/CloudStorage/OneDrive-Uni...   vac       4.0   \n",
       "11  /Users/jonas/Library/CloudStorage/OneDrive-Uni...   vac       4.0   \n",
       "\n",
       "         name  \n",
       "17  off_3.wav  \n",
       "3   air_3.wav  \n",
       "8   vac_1.wav  \n",
       "9   vac_0.wav  \n",
       "11  vac_3.wav  "
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ab9504",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.drop(df_test[(df_test.duration < 3.9)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb90603",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test[~df_test.name.str.contains(\"_0.wav\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788a1993",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = torch.stack([Transform.load_audio(fp) for fp in df_test.filepath])\n",
    "ytest = list(df_test.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce19d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = ApplianceDS(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f80a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=len(test_ds), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4804267b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([cats.index(val) for val in ytest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459cbee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1])"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds, *rest = classifier(Xtest)\n",
    "test_preds.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c458bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = ApplianceDS(Xtrain, ytrain)\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=len(test_ds), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a29bc1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5000)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([cats.index(val) for val in ytest])\n",
    "test_preds, *rest = classifier(Xtest)\n",
    "test_preds.argmax(axis=1)\n",
    "(torch.tensor([cats.index(val) for val in ytest]) == test_preds.argmax(axis=1)).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11db49f8",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dec60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path.name\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_name = str(path.name)\n",
    "time = datetime.today().strftime('%Y_%m_%d__%H_%M')\n",
    "torch.save(classifier, f\"{classifier_name}_finetuned_model_{time}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f95be15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['air', 'vac', 'off']"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea52913b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
